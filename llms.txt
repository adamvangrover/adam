# Adam: Project Context for LLMs

## Project Summary
Adam is a Neuro-Symbolic Financial Sovereign (v26.0) designed to provide calculated conviction for high-stakes capital allocation by fusing a Neural Swarm (System 1) with a Neuro-Symbolic Graph (System 2).

**Latest Version:** v26.0
**License:** MIT
**Documentation:** [Link to full docs]

## Why use Adam?
*   **Problem:** Standard LLMs hallucinate and lack the rigor required for institutional financial due diligence.
*   **Solution:** Adam implements a "System 2" architecture using a Neuro-Symbolic Planner that "thinks before it speaks," drafting, critiquing, and refining analysis.
*   **Key Differentiator:** Bifurcation of "Fast" (Swarm) and "Slow" (Graph) cognitive paths, ensuring both real-time responsiveness and deep analytical depth.

## Installation
```bash
# Standard installation
git clone https://github.com/adamvangrover/adam.git
cd adam
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync
source .venv/bin/activate
```

## Core Concepts & Terminology
*   **MetaOrchestrator:** The central nervous system that routes queries to the appropriate cognitive engine (Swarm, Deep Dive, etc.).
*   **System 1 (Swarm):** Asynchronous, parallelized "fire-and-forget" agents for perception and news monitoring (Fast Path).
*   **System 2 (Graph):** Stateful, directed acyclic graphs (DAGs) for complex reasoning, planning, and criticism (Slow Path).
*   **Deep Dive:** A comprehensive 5-phase financial analysis workflow (Entity, Fundamentals, Peers, Credit/Risk, Synthesis).
*   **Ingestion Engine:** Scalable data pipeline supporting in-memory (10MB), persistent (1GB), and distributed (100GB+) ingestion strategies.
*   **MCP:** Model Context Protocol used for tool integration.

## Usage Examples (Few-Shot Prompts)

### Example 1: Basic Initialization & Query
Initialize the Meta Orchestrator and run a simple market query.
```python
import asyncio
from core.engine.meta_orchestrator import MetaOrchestrator

async def main():
    # Initialize the brain
    orchestrator = MetaOrchestrator()

    # Execute a simple query (routed automatically)
    result = await orchestrator.route_request("What is the current price of AAPL?")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### Example 2: Triggering a Deep Dive
Perform a comprehensive analysis on a specific ticker using the "Deep Dive" trigger.
```python
import asyncio
from core.engine.meta_orchestrator import MetaOrchestrator

async def main():
    orchestrator = MetaOrchestrator()

    # "Deep dive" keyword triggers the System 2 Neuro-Symbolic Graph
    # Context can be used to inject specific parameters
    context = {"force_reflection": True}
    result = await orchestrator.route_request("Perform a deep dive analysis on TSLA", context=context)
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### Example 3: Swarm Intelligence
Use the async swarm to gather information in parallel.
```python
import asyncio
from core.engine.meta_orchestrator import MetaOrchestrator

async def main():
    orchestrator = MetaOrchestrator()

    # "Swarm" keyword activates the Hive Mind for parallel data gathering
    result = await orchestrator.route_request("Swarm scan for breaking news on NVIDIA")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### Example 4: Scalable Data Ingestion
Ingest large datasets using the tiered Ingestion Engine.
```python
from core.data_processing.ingestion_engine import IngestionEngine

# Initialize engine (Auto-detects strategy based on data size)
engine = IngestionEngine(mode="auto")

# 1. Ingest Small Data (In-Memory Graph)
small_data = [{"symbol": "AAPL", "sector": "Technology"}]
result_mem = engine.ingest(small_data)
print(f"Memory Ingestion: {result_mem}")

# 2. Ingest Large Corpus (Persistent Vector Store)
# Automatically chunks and embeds large files
result_large = engine.ingest("data/raw/sec_filings_2024.txt")
print(f"Persistent Ingestion: {result_large}")
```

## API Reference (Condensed)

### `core.engine.meta_orchestrator.MetaOrchestrator`
*   `__init__(legacy_orchestrator=None)`: Initializes the orchestrator and sub-engines (Planner, HiveMind, SemanticRouter).
*   `route_request(query: str, context: Optional[Dict] = None) -> Any`:
    *   Analyzes query complexity (Low, Medium, High, Deep Dive, Swarm, etc.).
    *   Routes to appropriate engine.
    *   Returns the result (Dict or String).

## Best Practices for LLMs
*   **Async/Await:** Adam is fundamentally asynchronous. Always use `await` when calling `route_request`.
*   **Context Injection:** Use the `context` dictionary to pass meta-parameters like `force_reflection`, `force_route`, or `repo_context`.
*   **Keywords:** The `MetaOrchestrator` uses keyword heuristics (e.g., "deep dive", "swarm", "red team") to route queries efficiently. Include these in your query string for deterministic routing.
*   **Error Handling:** Wrap `route_request` calls in `try/except` blocks as network operations or agent failures can occur.
