{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam v21.0: Interactive Pipeline Runner\n",
    "\n",
    "This notebook provides an interactive, step-by-step guide to executing the full three-stage training pipeline for the Adam v21.0 agent. It allows for the standalone testing and training of the model adapters."
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Note:** All underlying Python scripts (`.py` files) in the `tinker_upgrade/` directory have been refactored to use `asyncio` and the Tinker SDK's asynchronous API. This ensures a high-performance, non-blocking pipeline that properly overlaps network requests to maximize training efficiency and avoid missing GPU cycles. While the notebook executes these scripts via simple shell commands, the core logic is fully optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "The following command will create a Python virtual environment, install all necessary dependencies (including the Tinker SDK), and clone the `tinker-cookbook` for advanced training recipes. This step only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash tinker_upgrade/setup_env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** After running the setup script, you must activate the virtual environment. In your terminal, run:\n",
    "\n",
    "```bash\n",
    "source venv-tinker/bin/activate\n",
    "```\n",
    "\n",
    "Then, restart the Jupyter kernel and proceed with the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure API Key\n",
    "\n",
    "Before connecting to the Tinker service, you must add your API key to the `.env` file in the root of this project. Open the file and replace the placeholder with your actual key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Connection\n",
    "\n",
    "This script will use the API key from your `.env` file to connect to the Tinker API and list the available base models, confirming that your environment is correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/check_connection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Stage 1 - Tool Mastery (Neo4j Cypher)\n",
    "\n",
    "This stage fine-tunes the 'Workhorse' model (`Llama-3.1-8B`) to translate natural language questions into Neo4j Cypher queries. This gives the agent its 'Hands' to interact with the knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a: Generate the Text-to-Cypher Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/stage1_tool_use_gen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b: Train the Cypher LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/stage1_train_cypher.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Stage 2 - Cognitive Distillation\n",
    "\n",
    "This stage uses a powerful 'Mentor' model (`Qwen-235B`) to generate a high-quality dataset of financial analysis based on behavioral economics principles. The 'Workhorse' model is then trained on this data to distill the Mentor's reasoning capabilities into its own weights, giving the agent its 'Mind'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a: Generate Synthetic Data from the 'Mentor' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/stage2_create_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b: Train the 'Student' Model on the Distilled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/stage2_train_student.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Stage 3 - Meta-Cognitive Alignment (DPO)\n",
    "\n",
    "This final stage aligns the agent's 'Mind' with the firm's specific risk philosophy using Direct Preference Optimization (DPO). The model learns to prefer contrarian, data-driven analysis over consensus narratives, giving the agent its 'Soul'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a: Generate the DPO Preference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/stage3_dpo_prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b: Train the Final Adapter via DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/stage3_train_dpo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Merge Adapters for Production\n",
    "\n",
    "This crucial final step takes the sequentially trained `adam_aligned_soul_v1` adapter (which contains the combined knowledge of the 'Mind' and 'Soul') and formally designates it as the final, production-ready artifact: `adam_final_agent_lora`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/merge_adapters.py"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Final Adapters\n",
    "\n",
    "With all adapters trained and the final agent merged, this script downloads them to the `production_adapters` directory for local deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tinker_upgrade/download_adapters.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete: Final Artifacts Summary\n",
    "\n",
    "All training and merging steps are complete. The following deployable artifacts are now available in Tinker cloud storage:\n",
    "\n",
    "### 1. `adam_cypher_lora_v1`\n",
    "- **ROLE**: Specialist Tool Use Agent (The Hands)\n",
    "- **USAGE**: Load on-demand for translating natural language to Neo4j Cypher queries.\n",
    "\n",
    "### 2. `adam_final_agent_lora`\n",
    "- **ROLE**: Composed Reasoning & Alignment Agent (Mind + Soul)\n",
    "- **USAGE**: The primary, general-purpose adapter for financial analysis, behavioral reasoning, and generating final, aligned responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Full Pipeline Script\n",
    "\n",
    "For automated execution, the `run_full_pipeline.sh` script orchestrates all the steps above in a single command. It can be run from the terminal (with the virtual environment active) as follows:\n",
    "\n",
    "```bash\n",
    "bash tinker_upgrade/run_full_pipeline.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
