# --- Installation ---
# In Google Colab or Jupyter Notebook:
%pip install ipywidgets transformers torch sentencepiece accelerate google-generativeai nltk textstat -q

# --- Imports and Setup ---
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML as IPHTML
import time
import torch
import os
import re
import traceback # For printing errors more clearly

# Import the pipeline function from transformers
from transformers import pipeline, set_seed
from transformers.pipelines.base import PipelineException

# Import Google AI (handling key presence)
import google.generativeai as genai
gemini_available = False
gemini_model = None
try:
    # Attempt to load from Colab secrets first
    from google.colab import userdata
    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
    print("Attempting to load Google AI API Key from Colab secrets...")
except (ImportError, userdata.SecretNotFoundError):
    # Fallback to environment variable if not in Colab or secret not found
    print("Colab secrets not found or not in Colab. Checking environment variable 'GOOGLE_API_KEY'...")
    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')

if GOOGLE_API_KEY:
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        # Test with a simple call to ensure the key is valid (optional but good)
        # genai.list_models()
        print("Google AI API Key configured successfully.")
        gemini_available = True
    except Exception as e:
        print(f"Error configuring Google AI API Key: {e}. API features disabled.")
        gemini_available = False
        gemini_model = None
else:
    print("Google AI API Key not found in Colab secrets or environment variables.")
    print("Non-API methods will be used for Email/Expand/Simplify.")
    gemini_available = False
    gemini_model = None


# --- NLTK Setup ---
import nltk
nltk_ready = False
try:
    # Define path for NLTK data
    nltk_data_path = os.path.join(os.path.expanduser("~"), "nltk_data")
    if not os.path.exists(nltk_data_path):
        os.makedirs(nltk_data_path)
    if nltk_data_path not in nltk.data.path:
        nltk.data.path.append(nltk_data_path)

    # Function to check and download NLTK resources
    def download_nltk_resource(resource_id, resource_name):
        try:
            nltk.data.find(resource_id)
            print(f"NLTK resource '{resource_name}' found.")
            return True
        except LookupError:
            print(f"Downloading NLTK '{resource_name}' data...")
            try:
                nltk.download(resource_name, quiet=True, download_dir=nltk_data_path)
                nltk.data.find(resource_id) # Verify download
                print(f"'{resource_name}' downloaded successfully.")
                return True
            except Exception as download_error:
                print(f"\n--- !!! FAILED TO DOWNLOAD NLTK resource '{resource_name}' !!! ---")
                print(f"Error: {download_error}")
                print("---------------------------------------------------------------\n")
                return False

    # Download required resources
    resources_to_check = {
        'tokenizers/punkt': 'punkt',
        'taggers/averaged_perceptron_tagger': 'averaged_perceptron_tagger',
        'corpora/wordnet': 'wordnet',
        'corpora/stopwords': 'stopwords'
    }
    nltk_ready = all(download_nltk_resource(rid, rname) for rid, rname in resources_to_check.items())

    if nltk_ready:
        from nltk.tokenize import word_tokenize, sent_tokenize
        from nltk.tag import pos_tag
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer
        import string
        lemmatizer = WordNetLemmatizer()
        stop_words = set(stopwords.words('english'))
        print("NLTK components loaded successfully.")
    else:
        print("One or more NLTK resources failed to download/load.")
        print("NLTK-enhanced features will fallback to basic templates.")

except ImportError:
    print("NLTK library not found. Please ensure it is installed.")
    print("NLTK-enhanced features will fallback to basic templates.")
    nltk_ready = False
except Exception as nltk_setup_e:
    print(f"An unexpected error occurred during NLTK setup: {nltk_setup_e}")
    print("NLTK-enhanced features will fallback to basic templates.")
    nltk_ready = False


# Import Textstat safely
try:
    import textstat
    textstat_ready = True
    print("Textstat loaded successfully.")
except ImportError:
    print("Textstat library not found. Readability score feature disabled.")
    textstat_ready = False


# --- Setup Summarization Pipeline ---
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Setting up Summarizer pipeline. Using device: {device}")
summarizer = None
try:
    summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6", device=device)
    print("Summarization pipeline loaded successfully.")
except Exception as e:
    print(f"---!!! Error loading Summarization pipeline !!!---")
    print(f"Error: {e}")
    print(traceback.format_exc())
    print("Summarization feature will use basic text extraction fallback.")
    print("--------------------------------------------------")


# --- Setup Local AI Text Generation Pipeline (Load on demand) ---
local_text_generator = None # None: Not loaded, False: Load Failed, Pipeline obj: Loaded
local_generator_model_name = "t5-small"
local_generator_device = "cuda" if torch.cuda.is_available() else "cpu" # T5 often better on CUDA/CPU

def get_local_text_generator():
    """Loads the local text generator pipeline if not already attempted/loaded."""
    global local_text_generator
    if local_text_generator is None: # Only attempt loading if it's None (initial state)
        print(f"\nAttempting to load local AI model ({local_generator_model_name})...")
        print(f"This may take time and consume resources. Device: {local_generator_device}")
        status_label.value = f"Status: Loading local AI model ({local_generator_model_name})..."
        try:
            # Using text2text-generation for T5
            local_text_generator = pipeline('text2text-generation', model=local_generator_model_name, device=local_generator_device)
            print(f"Local AI model '{local_generator_model_name}' loaded successfully.")
            status_label.value = "Status: Local AI model ready."
            return local_text_generator # Return the pipeline object
        except Exception as e:
            print(f"\n---!!! FAILED TO LOAD Local AI Model '{local_generator_model_name}' !!!---")
            print(f"Error: {e}")
            print(traceback.format_exc())
            print("Local AI Generation will not be available. Falling back.")
            print("-----------------------------------------------------------\n")
            status_label.value = "Status: Error loading Local AI Model!"
            local_text_generator = False # Set to False to indicate loading failed
            return None # Return None on failure
    elif local_text_generator == False:
        # If loading previously failed, don't retry
        return None
    else:
        # If already loaded, return the pipeline object
        return local_text_generator


# --- Core Logic Functions ---

# (generate_summary - minor cleanup)
MIN_WORDS_FOR_SUMMARY = 15
def generate_summary(text):
    """Summarizes text using Transformers or basic fallback."""
    if not text or not text.strip(): return "* No text provided."
    word_count = len(text.split());
    if word_count < MIN_WORDS_FOR_SUMMARY: return f"* Input too short (needs >{MIN_WORDS_FOR_SUMMARY} words)."
    if not summarizer: # Check if summarizer loaded correctly
        print("Summarizer unavailable, using basic extraction.")
        return "* Summarizer unavailable.\n* " + "\n* ".join([s.strip() for s in text.split('.') if s.strip()][:3])

    print("Generating summary with Transformers model...")
    status_label.value = "Status: Summarizing (model running)..."
    try:
        # Ensure text is not excessively long before tokenization (optional safeguard)
        # text = text[:20000] # Limit input length if needed
        max_input_length = summarizer.tokenizer.model_max_length
        input_ids = summarizer.tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=max_input_length).to(summarizer.device)
        input_length = input_ids.shape[1]
        min_summ_len = max(15, input_length // 10)
        max_summ_len = max(45, input_length // 3)
        max_summ_len = max(max_summ_len, min_summ_len + 15)

        # Decode input_ids back to string for the pipeline if needed by specific versions
        decoded_input = summarizer.tokenizer.decode(input_ids[0], skip_special_tokens=True)

        summary_result = summarizer(decoded_input, max_length=max_summ_len, min_length=min_summ_len, do_sample=False)

        if summary_result and isinstance(summary_result, list) and summary_result[0].get('summary_text'):
            summary_text = summary_result[0]['summary_text']
            bullets = [f"* {s.strip()}." for s in summary_text.split('.') if s.strip()]
            # Handle case where splitting results in no bullets but text exists
            if not bullets and summary_text.strip():
                 bullets = [f"* {summary_text.strip()}."]
            return "\n".join(bullets) if bullets else "* Model returned empty summary."
        else:
            print(f"Warning: Unexpected summary result format: {summary_result}")
            return "* Failed to generate summary (unexpected model output)."
    except Exception as e:
        print(f"---!!! Error during Summarization Pipeline !!!---")
        print(f"Error: {e}")
        print(traceback.format_exc())
        print("----------------------------------------------")
        return f"* Error during summarization: Check input or model. ({type(e).__name__})"


# --- NLTK Helper ---
def extract_keywords(text, num_keywords=5):
    """Extract simple keywords (nouns/verbs) using NLTK. Robust fallback."""
    if not nltk_ready: return [] # Return empty list if NLTK failed setup
    try:
        words = word_tokenize(text.lower())
        # Added check for words list
        if not words: return []
        tagged = pos_tag([word for word in words if word.isalnum() and word not in stop_words])
        if not tagged: return []
        keywords = [lemmatizer.lemmatize(word) for word, tag in tagged if tag.startswith('NN') or tag.startswith('VB')]
        if not keywords: return []
        freq = nltk.FreqDist(keywords)
        return [kw for kw, count in freq.most_common(num_keywords)]
    except Exception as e:
        # Catch potential errors during NLTK processing (e.g., LookupError if download failed despite checks)
        print(f"NLTK Error during keyword extraction: {e}. Returning empty keywords.")
        return []


# --- Generation Functions (API or Selected Non-API Method) ---

def call_gemini_api(prompt):
    """Helper function to call the Gemini API and handle errors."""
    if not gemini_available or not gemini_model: return "Error: Google AI API not available/configured."
    try:
        status_label.value = "Status: Calling Google AI API..."
        response = gemini_model.generate_content(prompt)
        status_label.value = "Status: Received response from API."
        # Consider adding safety feedback checks if needed:
        # if response.prompt_feedback.block_reason:
        #    return f"Error: Blocked by API Safety Filter ({response.prompt_feedback.block_reason})"
        # Access text safely
        return response.text
    except Exception as e:
        print(f"---!!! Error calling Google AI API !!!---")
        print(f"Error: {e}")
        print(traceback.format_exc())
        print("----------------------------------------")
        status_label.value = "Status: API Error."
        return f"Error interacting with Google AI API: {type(e).__name__}"

def call_local_transformer_generation(prompt, max_len=150):
    """Calls the local text generation model with robust error handling."""
    generator = get_local_text_generator() # Attempt to load/get model
    if not generator:
        return "Error: Local AI model unavailable or failed to load."

    print(f"Generating text with local model ({local_generator_model_name})...")
    status_label.value = f"Status: Generating with Local AI ({local_generator_model_name})..."
    try:
        results = generator(prompt, max_length=max_len, num_return_sequences=1, clean_up_tokenization_spaces=True, truncation=True) # Added truncation
        if results and isinstance(results, list) and results[0].get('generated_text'):
             generated = results[0]['generated_text']
             # Basic post-processing (remove excessive newlines etc.)
             generated = re.sub(r'\n\s*\n', '\n\n', generated).strip()
             return generated
        else:
             print(f"Warning: Unexpected local generator result format: {results}")
             return "Error: Local AI model returned empty/unexpected output."
    except Exception as e:
        print(f"\n---!!! ERROR during Local AI Generation !!!---")
        print(f"Prompt (start): {prompt[:200]}...")
        print(f"Error: {e}")
        print(traceback.format_exc())
        print("------------------------------------------------\n")
        status_label.value = "Status: Error during Local AI generation!"
        return f"Error during local AI generation: {type(e).__name__}"


# --- Enhanced Template Functions (Ultimate Fallbacks) ---
def template_email(bullets):
     print("Using basic template for email generation.")
     first_bullet = bullets.split('\n')[0].lstrip('*- ').strip(); subject_guess = " ".join(first_bullet.split()[:10]); subject_line = f"Subject: Update on {subject_guess}..." if subject_guess else "Subject: Summary"
     return f"{subject_line}\n\nTo: [Recipient]\nFrom: [Your Name]\n\nHi Team,\n\nPoints:\n{bullets}\n\nBest,\n[Your Name]\n\n(Note: Basic template used as fallback.)"
def template_expand(bullets):
     print("Using basic template for expansion.")
     points = [p.strip().lstrip('*- ') for p in bullets.split('\n') if p.strip()]; expanded = "Expanded Points (Basic Placeholder):\n\n" + "\n\n".join([f"**{pt}:**\n   - [Placeholder elaboration...]" for pt in points])
     return expanded + "\n\n(Note: Basic template used as fallback.)"
def template_simplify(bullets):
     print("Using basic template for simplification.")
     points = [p.strip().lstrip('*- ') for p in bullets.split('\n') if p.strip()]; simplified = "Simple Explanation (Basic Placeholder):\n\n" + "\n".join([f"* **{pt}:** This means [simple interpretation...]." for pt in points])
     return simplified + "\n\n(Note: Basic template used as fallback.)"

# --- Main Generation Functions with Selection Logic ---

# Helper to select and run non-API method with fallback
def run_non_api_method(method_name, fn_args, fallback_template_fn):
    """Runs the selected non-API method with fallbacks."""
    method = non_api_method_selector.value
    result = None
    executed_method = "Template Fallback" # Assume fallback initially

    try:
        if method == 'Local AI Model (Experimental)':
            executed_method = 'Local AI Model'
            # Prepare prompt based on method_name
            bullets = fn_args['bullets']
            custom_prompt = custom_prompt_input.value.strip()
            default_prompts = {
                'email': f"Generate a short, professional email incorporating these key points:\n\n{bullets}\n\nEmail:",
                'expand': f"Expand on the following points, providing more detail for each:\n\n{bullets}\n\nExpansion:",
                'simplify': f"Explain the following points in simple terms:\n\n{bullets}\n\nExplanation:"
            }
            max_lens = {'email': 250, 'expand': 350, 'simplify': 250}
            prompt = custom_prompt if custom_prompt else default_prompts[method_name]
            # Add task prefix for T5 if needed (can be refined)
            if "t5" in local_generator_model_name:
                 task_prefixes = {'email': "write an email based on:", 'expand': "expand these points:", 'simplify': "explain in simple terms:"}
                 prompt = f"{task_prefixes[method_name]} {prompt}"

            result = call_local_transformer_generation(prompt, max_len=max_lens[method_name])

        elif method == 'NLTK Enhanced' and nltk_ready:
            executed_method = 'NLTK Enhanced'
            # Call the specific NLTK-enhanced function
            if method_name == 'email': result = run_nltk_email(fn_args['bullets'])
            elif method_name == 'expand': result = run_nltk_expand(fn_args['bullets'], fn_args['original_text'])
            elif method_name == 'simplify': result = run_nltk_simplify(fn_args['bullets'])

        # If result is still None (NLTK failed internally or method was 'Enhanced Templates')
        # or if the execution resulted in an error string
        if result is None or (isinstance(result, str) and result.startswith("Error:")):
            if result and result.startswith("Error:"): print(f"Execution of '{executed_method}' failed, using template fallback.")
            result = fallback_template_fn(fn_args['bullets']) # Use specific fallback template
            executed_method = "Template Fallback"

        return result, executed_method # Return result and which method succeeded

    except Exception as e:
        print(f"---!!! ERROR in run_non_api_method ({method_name}, method: {method}) !!!---")
        print(f"Error: {e}")
        print(traceback.format_exc())
        print("--------------------------------------------------------------------------")
        status_label.value = f"Status: Error running {method_name}!"
        # Ultimate fallback to basic template
        return fallback_template_fn(fn_args['bullets']), "Template Fallback (Error)"


# --- Specific NLTK implementations (called by run_non_api_method) ---
def run_nltk_email(bullets):
    """Generates email using NLTK-enhanced template."""
    print("Using NLTK-enhanced template for email generation.")
    all_keywords = extract_keywords(bullets, num_keywords=3); keyword_str = ", ".join(all_keywords) if all_keywords else "key topics"
    first_bullet = bullets.split('\n')[0].lstrip('*- ').strip(); subject_guess = " ".join(first_bullet.split()[:10]); subject_line = f"Subject: Update regarding {keyword_str}..." if keyword_str != "key topics" else "Subject: Summary"
    return f"{subject_line}\n\nTo: [Recipient]\nFrom: [Your Name]\n\nHi Team,\nFollowing up regarding {keyword_str}:\n{bullets}\n\nBest,\n[Your Name]\n\n(Note: NLTK-enhanced template.)"

def run_nltk_expand(bullets, original_text):
    """Expands bullets using NLTK-based extractive method."""
    print("Using NLTK extractive method for expansion.")
    if not original_text: return "Error: Original text needed for non-API 'Expand' (NLTK Extractive)."
    expanded_text = "Expanded Points (Extracted using NLTK):\n\n"; points = [p.strip() for p in bullets.split('\n') if p.strip()];
    try: original_sentences = sent_tokenize(original_text)
    except Exception as e: print(f"NLTK Error tokenizing original text: {e}"); return "Error: Could not parse sentences from original text."
    if not original_sentences: return "Error: No sentences found in original text."

    for i, point in enumerate(points):
        clean_point = point.lstrip('*- ').strip(); expanded_text += f"**{i+1}. Regarding: '{clean_point}'**\n"
        bullet_keywords = set(extract_keywords(clean_point, num_keywords=5));
        if not bullet_keywords: bullet_keywords = set(w for w in word_tokenize(clean_point.lower()) if w.isalnum()) # Fallback keywords
        sentence_scores = {};
        for sent in original_sentences:
             try: sent_words = set(w for w in word_tokenize(sent.lower()) if w.isalnum()); overlap = len(bullet_keywords.intersection(sent_words));
             except Exception as e: print(f"Warn: Error tokenizing sentence: {sent[:50]}... Error: {e}"); continue # Skip problematic sentence
             if overlap > 0: sentence_scores[sent] = overlap
        sorted_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True); limit = 3; found_count = 0
        for sent, score in sorted_sentences[:limit]:
             try: # Add distance check in try-except as well
                 if nltk.edit_distance(clean_point.lower(), sent.lower()) > len(clean_point) * 0.4: expanded_text += f"   - [Found Context]: {sent}\n"; found_count +=1
             except Exception as e: print(f"Warn: Error calculating edit distance. {e}"); continue
        if found_count == 0: expanded_text += "   - [No specific related sentences found in original text.]\n"
        expanded_text += "\n"
    return expanded_text + "\n(Note: NLTK extractive method used.)"

def run_nltk_simplify(bullets):
    """Explains bullets simply using NLTK/Textstat-enhanced template."""
    print("Using NLTK/Textstat enhanced template for simplification.")
    readability_info = ""
    if textstat_ready:
        try:
            flesch_score = textstat.flesch_reading_ease(bullets)
            # **FIXED SyntaxError HERE**
            if flesch_score > 90: readability = "very easy"
            elif flesch_score > 70: readability = "easy"
            elif flesch_score > 60: readability = "fairly easy"
            elif flesch_score > 50: readability = "standard"
            elif flesch_score > 30: readability = "fairly difficult"
            else: readability = "very difficult"
            readability_info = f"(Readability Score: {flesch_score:.1f} - {readability})\n"
        except Exception as e: print(f"Textstat readability calculation failed: {e}")
    else:
        readability_info = "(Textstat not available)\n"

    simple_explanation = f"Simple Explanation (NLTK/Textstat Enhanced):\n{readability_info}\n"; points = [p.strip() for p in bullets.split('\n') if p.strip()]
    for i, point in enumerate(points):
        clean_point = point.lstrip('*- ').strip(); keywords = extract_keywords(clean_point, num_keywords=3); keyword_str = f" (Key terms: {', '.join(keywords)})" if keywords else ""
        simple_explanation += f"**{i+1}. Point: '{clean_point}'**{keyword_str}\n"; simple_explanation += f"   - **In Simple Terms:** [Explain the core idea simply].\n"; simple_explanation += f"   - **Why it Matters:** [Explain significance briefly].\n\n"
    return simple_explanation + "\n(Note: NLTK/Textstat template used.)"


# --- Main Action Functions Calling the Logic ---
def generate_email_from_bullets(bullets):
    if gemini_available: return call_gemini_api(f"Generate a professional email draft incorporating these points:\n\n{bullets}\n\nEmail Draft:")
    else: return run_non_api_method('email', {'bullets': bullets}, template_email)

def expand_bullets_text(bullets, original_text):
    if gemini_available: return call_gemini_api(f"Expand on these bullet points, providing detail/context:\n\n{bullets}\n\nExpanded Version:")
    else: return run_non_api_method('expand', {'bullets': bullets, 'original_text': original_text}, template_expand)

def explain_bullets_simply(bullets):
    if gemini_available: return call_gemini_api(f"Explain these points in simple, clear language:\n\n{bullets}\n\nSimple Explanation:")
    else: return run_non_api_method('simplify', {'bullets': bullets}, template_simplify)


# --- UI Elements ---
# (Largely unchanged, ensure descriptions match functionality)
large_text_input = widgets.Textarea(placeholder='Paste text (ideally >15 words)...', layout=widgets.Layout(width='95%', height='200px'), description='Input Text:')
clear_large_text_button = widgets.Button(description="Clear Input", button_style='warning', icon='eraser', layout=widgets.Layout(width='auto'))
large_text_hbox = widgets.HBox([large_text_input, clear_large_text_button])
summarize_button = widgets.Button(description='Summarize to Bullets', button_style='info', tooltip='Summarize text (uses Transformers or fallback)', icon='list')
auto_fill_checkbox = widgets.Checkbox(value=True, description='Auto-fill bullets below?', indent=False)
summary_controls = widgets.HBox([summarize_button, auto_fill_checkbox])
summary_output_display = widgets.HTML(value="<p>Summary will appear here...</p>", layout=widgets.Layout(width='95%', min_height='80px', border='1px solid lightgray', padding='5px', overflow_y='auto'))
bullet_input = widgets.Textarea(placeholder='Paste or edit bullet points here...', layout=widgets.Layout(width='95%', height='150px'), description='Bullets:')
clear_bullets_button = widgets.Button(description="Clear Bullets", button_style='warning', icon='eraser', layout=widgets.Layout(width='auto'))
bullet_input_hbox = widgets.HBox([bullet_input, clear_bullets_button])
email_button = widgets.Button(description='Generate Email', button_style='success', tooltip='Turn bullets into email', icon='envelope')
expand_button = widgets.Button(description='Expand Bullets', button_style='', tooltip='Elaborate on bullets', icon='expand')
simplify_button = widgets.Button(description='Explain Simply', button_style='', tooltip='Explain bullets simply', icon='question-circle')
bullet_buttons = widgets.HBox([email_button, expand_button, simplify_button])
results_output_display = widgets.HTML(value="<p>Results will appear here...</p>", layout=widgets.Layout(width='95%', min_height='100px', border='1px solid lightgray', padding='5px', overflow_y='auto'))
status_label = widgets.Label(value="Status: Initializing...") # Initial status

# --- Non-API Method Selector & Custom Prompt ---
non_api_method_selector = widgets.RadioButtons(
    options=['Enhanced Templates', 'NLTK Enhanced', 'Local AI Model (Experimental)'],
    value='Enhanced Templates', # Default
    description='Non-API Method:',
    disabled=gemini_available
)
custom_prompt_input = widgets.Textarea(
    placeholder='(Optional) Enter custom prompt for "Local AI Model" method.',
    layout=widgets.Layout(width='95%', height='80px', display='none'), # Start hidden
    description='Custom Prompt:'
)

# Observer to show/hide custom prompt box
def handle_method_change(change):
    # Show prompt box only if API is OFF *and* Local AI is selected
    show_prompt = (not gemini_available) and (change['new'] == 'Local AI Model (Experimental)')
    custom_prompt_input.layout.display = 'flex' if show_prompt else 'none'
non_api_method_selector.observe(handle_method_change, names='value')


# --- Callback Functions ---
def clear_widget_value(b, widget): widget.value = ''
clear_large_text_button.on_click(lambda b: clear_widget_value(b, large_text_input))
clear_bullets_button.on_click(lambda b: clear_widget_value(b, bullet_input))

def on_summarize_button_clicked(b):
    status_label.value = "Status: Preparing summary..."; summary_output_display.value = "<p><i>Processing summary...</i></p>"
    input_text = large_text_input.value; summary_bullets = generate_summary(input_text)
    escaped_summary = summary_bullets.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;'); summary_output_display.value = f"<pre>{escaped_summary}</pre>"
    # Update status based on outcome
    if not summary_bullets.startswith("*"):
        status_label.value = "Status: Summary generated."
        if auto_fill_checkbox.value:
            bullet_input.value = summary_bullets
            status_label.value += " Auto-filled." # Append auto-fill status
    else:
        status_label.value = f"Status: Summary {summary_bullets[2:]}" # Display the error from generate_summary


# Generic handler - Simplified status update logic
def handle_bullet_action(action_func, button_description, requires_original_text=False):
    status_label.value = f"Status: Preparing '{button_description}'..."
    results_output_display.value = f"<p><i>Processing {button_description} request...</i></p>"
    input_bullets = bullet_input.value
    if not input_bullets.strip(): results_output_display.value = "<p style='color:red;'>Need bullet points.</p>"; status_label.value = "Status: Error - No bullets."; return

    original_text = large_text_input.value if requires_original_text else None
    executed_method = "API" if gemini_available else "Non-API" # Placeholder for status

    # Call action function
    try:
        if requires_original_text:
            result_data = action_func(input_bullets, original_text)
        else:
            result_data = action_func(input_bullets)

        # Unpack result if it's from run_non_api_method
        if isinstance(result_data, tuple) and len(result_data) == 2:
             result_text, executed_method = result_data
        else: # Assumed API call or direct return
             result_text = result_data

    except Exception as e:
         print(f"---!!! UNCAUGHT ERROR in handle_bullet_action ({button_description}) !!!---")
         print(f"Error: {e}")
         print(traceback.format_exc())
         print("-----------------------------------------------------------------------")
         result_text = f"FATAL Error during '{button_description}': {type(e).__name__}. Check logs."
         executed_method = "Error"

    # Display result
    escaped_result = result_text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    results_output_display.value = f"<pre>--- {button_description} Result ---<br>{escaped_result}</pre>"

    # Update status
    final_status = f"Status: {button_description} "
    if result_text.startswith("Error:") or executed_method == "Error": final_status += "Error."
    elif gemini_available: final_status += "generated (API)."
    else: final_status += f"generated ({executed_method})." # Use the method reported by run_non_api_method
    status_label.value = final_status


# --- Link Callbacks ---
summarize_button.on_click(on_summarize_button_clicked)
email_button.on_click(lambda b: handle_bullet_action(generate_email_from_bullets, "Email Draft"))
expand_button.on_click(lambda b: handle_bullet_action(expand_bullets_text, "Expanded Text", requires_original_text=True))
simplify_button.on_click(lambda b: handle_bullet_action(explain_bullets_simply, "Simple Explanation"))

# --- Assemble Layout and Display ---
title = widgets.HTML("<h2>Interactive Text Processing Tool v6 (Robust)</h2>")
api_note_widget = widgets.HTML()
def update_api_note(): # Function to set API note correctly
    if gemini_available: api_note_widget.value = "<p style='color:green; font-weight:bold;'>✅ Google AI API Key found. Using Gemini API.</p>"
    else: api_note_widget.value = "<p style='color:orange; font-weight:bold;'>⚠️ Google AI API Key not found. Using selected Non-API Method below.</p>"
update_api_note()

# Controls for non-API shown only if API key is missing
non_api_controls = widgets.VBox([non_api_method_selector, custom_prompt_input])
non_api_controls.layout.display = 'none' if gemini_available else 'flex'
# Ensure custom prompt visibility is correct initially based on default selector value
custom_prompt_input.layout.display = 'none' if gemini_available or non_api_method_selector.value != 'Local AI Model (Experimental)' else 'flex'


description_text1 = widgets.HTML("<p>Paste text (>15 words), click 'Summarize' (uses Transformers/fallback).</p>")
description_text2 = widgets.HTML("<p>Paste/edit bullets. If no API key, select Non-API method & optionally add custom prompt for Local AI, then use buttons.</p>")

ui = widgets.VBox([
    title,
    api_note_widget,
    status_label,
    description_text1,
    large_text_hbox,
    summary_controls,
    summary_output_display,
    widgets.HTML("<hr>"),
    description_text2,
    non_api_controls, # Add the non-API selector/prompt group
    bullet_input_hbox,
    bullet_buttons,
    results_output_display
])

# --- Display the UI ---
print("Initializing UI...")
display(ui)
status_label.value = "Status: Ready." # Set final ready status after UI display
print("\n--- Initialization Complete ---")
print(f"Gemini API Available: {gemini_available}")
print(f"NLTK Ready: {nltk_ready}")
print(f"Textstat Ready: {textstat_ready}")
print("Summarizer Pipeline Loaded:", bool(summarizer))
print("Local AI Generator Pre-loaded:", bool(local_text_generator) and local_text_generator != False)
print("UI should be displayed above.")
