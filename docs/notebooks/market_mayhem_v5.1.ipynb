# -*- coding: utf-8 -*-
"""
Market Mayhem Newsletter Generator - v5.1 (Interactive UI)

- Fixes NameError from v5.0.
- Implements interactive update using ipywidgets after initial generation.
- User can see the first draft, input URLs/Text, and click Update.
- Maintains multi-layer data fetching & simulated AI synthesis.
"""

# --- 1. Setup: Install Libraries ---
!pip install yfinance requests beautifulsoup4 Jinja2 feedparser newspaper3k transformers torch sentencepiece alpha_vantage newsapi-python lxml[html_clean] ipywidgets -q
# Ensure ipywidgets is installed

# --- 2. Imports ---
import yfinance as yf
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone, timedelta
import time
import re
import os
import feedparser
from jinja2 import Environment, Template
from urllib.parse import urljoin, urlparse
import logging
from collections import defaultdict, Counter
try:
    from newspaper import Article, Config as NewspaperConfig
    NEWSPAPER_AVAILABLE = True
except ImportError as e_imp:
    logging.error(f"Newspaper3k import failed: {e_imp}. Article scraping will be disabled.")
    NEWSPAPER_AVAILABLE = False
import traceback

# API Clients (Optional)
from alpha_vantage.timeseries import TimeSeries
from newsapi import NewsApiClient

# Transformers & Display
from transformers import pipeline, logging as hf_logging
hf_logging.set_verbosity_error()
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output

# --- 3. Logging Setup ---
for handler in logging.root.handlers[:]: logging.root.removeHandler(handler)
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
logging.info("Libraries installed and imported. Logging configured.")

# --- 4. Configuration ---
# *** CRITICAL: REVIEW & UPDATE FEEDS AND SELECTORS BELOW ***
CONFIG = {
    'request_headers': {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}, # Try Googlebot UA
    'request_timeout': 30, # Increased timeout slightly
    'polite_delay': 0.8, # Faster delay
    'target_date_str': datetime.now(timezone.utc).strftime('%Y-%m-%d'),

    'api_keys': { # Set in Colab Secrets
        'alpha_vantage': 'ALPHA_VANTAGE_API_KEY',
        'newsapi': 'NEWS_API_KEY'
    },

    'indices': {'symbols': ['^GSPC', '^DJI', '^IXIC']},
    'movers': {
        'enabled': True,
        'scrape_gainers_url': "https://finance.yahoo.com/gainers?count=25",
        'scrape_losers_url': "https://finance.yahoo.com/losers?count=25",
        'scrape_selectors': { # *** USER MUST VERIFY THESE LIVE ***
             'rows': 'table > tbody > tr', # Primary guess
             'fallback_rows': 'tr.simpTblRow', # Fallback guess
             'symbol_cell_index': 0, 'name_cell_index': 1, 'change_percent_cell_index': 4,
             'symbol_tag_selector': 'a[data-test="quoteLink"]',
             'change_percent_tag_selector': 'fin-streamer[data-field="regularMarketChangePercent"] span' # Target span within streamer
        },
        'limit': 5
    },
    'data_spotlight': {'symbol': '^VIX', 'name': 'VIX (Fear Gauge)'},
    # ** Corrected key name and added missing key **
    'news_gathering': {
        'rss_feeds': { # Use working feeds + Google News searches (Verify these links!)
            'debt': ["https://news.google.com/rss/search?q=corporate+debt+OR+bond+market+OR+interest+rates+when:1d&hl=en-US&gl=US&ceid=US:en"],
            'risk': ["https://rss.nytimes.com/services/xml/rss/nyt/World.xml", "http://feeds.bbci.co.uk/news/world/rss.xml"],
            'deals': ["https://news.google.com/rss/search?q=M%26A+OR+acquisition+OR+merger+when:1d&hl=en-US&gl=US&ceid=US:en"],
            'trade': ["https://news.google.com/rss/search?q=trade+OR+tariff+OR+commerce+department+when:1d&hl=en-US&gl=US&ceid=US:en"],
            'commentary': ["https://feeds.a.dj.com/rss/RSSMarketsMain.xml", "https://www.investing.com/rss/news_25.rss"],
            'general': ["https://rss.nytimes.com/services/xml/rss/nyt/Business.xml", "http://feeds.bbci.co.uk/news/business/rss.xml"]
        },
        'article_sources': { # URLs whose top links we might try scraping with newspaper3k (Used for secondary search source URLs now)
             'debt': "https://news.google.com/rss/search?q=corporate+debt+OR+bond+market+when:1d&hl=en-US&gl=US&ceid=US:en",
             'risk': "https://news.google.com/rss/search?q=market+risk+OR+economic+outlook+when:1d&hl=en-US&gl=US&ceid=US:en",
             'commentary': "https://news.google.com/rss/search?q=market+commentary+OR+stock+analysis+when:1d&hl=en-US&gl=US&ceid=US:en",
        },
        'newspaper3k_enabled': NEWSPAPER_AVAILABLE,
        'max_items_per_primary_source': 5,
        'max_secondary_search_terms': 1,
        'max_items_per_secondary_search': 2,
        'max_articles_to_scrape_per_topic': 3,
        'min_article_text_length': 300,
        'keyword_extraction_stopwords': ['market', 'stock', 'shares', 'inc', 'corp', 'company', 'says', 'report', 'news', 'update', 'daily', 'investor', 'trading', 'percent', 'points', 'index'],
        # --- ADDED MISSING KEY ---
        'max_articles_to_process_per_topic': 5 # Max *total* text items to aim for per topic for AI input
    },
    'ai_processing': { # Settings for (simulated) AI
        'enabled': True,
        'sentiment_model': 'distilbert-base-uncased-finetuned-sst-2-english',
    },
     'output': {
         'newsletter_title': 'Market Mayhem - Daily Briefing v5.1', # Changed version here
         'sector_notes_placeholder': 'Sector performance analysis requires review.'
     }
}

# --- 5. Helper Functions (make_request, resolve_redirect, parse_rss_feed, scrape_article_text_newspaper, extract_keywords) ---
# ... (Keep these functions as defined and fixed in v4.2) ...
# Example: make_request from v4.2
def make_request(url, retries=1, delay=1, stream=False, allow_redirects=True):
    logging.debug(f"Requesting URL: {url}")
    for attempt in range(retries + 1):
        try:
            response = requests.get(url, headers=CONFIG['request_headers'], timeout=CONFIG['request_timeout'], stream=stream, allow_redirects=allow_redirects)
            logging.debug(f"Attempt {attempt+1}: Status Code {response.status_code} for {url}")
            if response.status_code >= 400:
                 level = logging.ERROR if attempt == retries or response.status_code in [403, 401, 404] else logging.WARNING
                 logging.log(level, f"Attempt {attempt+1}: HTTP {response.status_code} error fetching {url}")
                 if response.status_code in [403, 401, 404]: break
                 if response.status_code == 429: wait_time = delay * 5 * (2 ** attempt); logging.warning(f"Rate limited (429). Waiting {wait_time:.1f}s..."); time.sleep(wait_time); continue
            else: return response
        except requests.exceptions.Timeout: logging.warning(f"Attempt {attempt+1}: Timeout error fetching {url}")
        except requests.exceptions.TooManyRedirects: logging.warning(f"Attempt {attempt+1}: Too many redirects fetching {url}"); break
        except requests.exceptions.RequestException as e: logging.warning(f"Attempt {attempt+1}: Request exception for {url}: {e}")
        except Exception as e: logging.error(f"Attempt {attempt+1}: Unexpected request error for {url}: {e}", exc_info=True); break
        if attempt < retries:
            wait_time = delay * (2 ** attempt); logging.info(f"Retrying {url} in {wait_time:.1f} seconds..."); time.sleep(wait_time)
    logging.error(f"Failed to fetch URL {url} after {retries+1} attempts.")
    return None

def resolve_redirect(url):
    # ... (Keep v4.2 logic) ...
    if not url or not isinstance(url, str) or "news.google.com/rss/articles/" not in url: return url
    logging.debug(f"Attempting to resolve redirect for: {url}")
    try:
        response = requests.head(url, headers=CONFIG['request_headers'], allow_redirects=True, timeout=15)
        if 200 <= response.status_code < 400: final_url = response.url; logging.debug(f"Resolved redirect (HEAD): {url} -> {final_url}"); return final_url
        else:
            response_get = make_request(url, retries=0, allow_redirects=True)
            if response_get: final_url_get = response_get.url; logging.debug(f"Resolved redirect (GET): {url} -> {final_url_get}"); return final_url_get
            else: logging.warning(f"Redirect resolution failed for {url}"); return url
    except Exception as e: logging.warning(f"Error resolving redirect for {url}: {e}"); return url

def parse_rss_feed(feed_url, source_name, limit=7):
    # ... (Keep logic from v4.2, returning list of dicts or []) ...
    logging.info(f"Attempting RSS: {source_name} ({feed_url})"); items_for_ai = []
    try:
        response = make_request(feed_url);
        if not response: return []
        feed = feedparser.parse(response.content)
        if feed.bozo: logging.warning(f"Feedparser issue parsing {feed_url}: {feed.bozo_exception}")
        if not feed.entries: logging.warning(f"No entries found in RSS feed: {feed_url}"); return []
        for entry in feed.entries[:limit]:
            title = getattr(entry, 'title', None); original_link = getattr(entry, 'link', None)
            if not title or not original_link or len(title) < 15: continue
            content_html = ''; # Extract content logic (same as v4.2)
            if hasattr(entry, 'content') and entry.content: content_html = entry.content[0].value
            if not content_html and hasattr(entry, 'summary'): content_html = entry.summary
            if not content_html and hasattr(entry, 'description'): content_html = entry.description
            text_content = BeautifulSoup(content_html, "html.parser").get_text(separator=' ', strip=True) if content_html else None
            final_text = text_content if text_content and len(text_content) > 30 else title
            resolved_url = resolve_redirect(original_link) # Resolve here
            time.sleep(0.05) # Shorter delay for resolving only
            items_for_ai.append({'title': title, 'url': original_link, 'resolved_url': resolved_url, 'source': source_name, 'text': final_text})
        logging.info(f"Parsed {len(items_for_ai)} relevant items from RSS: {source_name}")
        return items_for_ai
    except Exception as e: logging.error(f"Failed to process RSS feed {feed_url}: {e}", exc_info=True); return []

def scrape_article_text_newspaper(url_to_scrape):
     # ... (Keep logic from v4.2, returning dict or None) ...
    if not CONFIG['news_gathering']['newspaper3k_enabled'] or not url_to_scrape: return None
    logging.info(f"Attempting Newspaper3k scrape on URL: {url_to_scrape}")
    try:
        parsed_link = urlparse(url_to_scrape);
        if not parsed_link.scheme.startswith('http'): return None
        news_config = NewspaperConfig(); news_config.browser_user_agent = CONFIG['request_headers']['User-Agent']
        news_config.request_timeout = CONFIG['request_timeout']; news_config.fetch_images = False
        news_config.memoize_articles = False; news_config.verbose = False
        article = Article(url_to_scrape, config=news_config); article.download(); article.parse()
        min_len = CONFIG['news_gathering']['min_article_text_length']
        if article.text and len(article.text) >= min_len:
            logging.info(f"Newspaper3k SUCCESS for: {url_to_scrape}")
            title = article.title if article.title else "Title N/A"; source_netloc = parsed_link.netloc.replace('www.', '')
            return {'source': source_netloc, 'title': title, 'scraped_text': article.text[:4000]}
        else: logging.warning(f"Newspaper3k: Text length ({len(article.text) if article.text else 0}) < {min_len} or parse failed for {url_to_scrape}"); return None
    except Exception as e: logging.warning(f"Newspaper3k failed for {url_to_scrape}: {e}"); return None

def extract_keywords(text, top_n=2):
     # ... (Keep logic from v4.2) ...
    if not text: return []
    words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
    stopwords = set(CONFIG['news_gathering']['keyword_extraction_stopwords']) | set(['https', 'http', 'com', 'www', 'news', 'google']) # Add common web noise
    meaningful_words = [word for word in words if word not in stopwords]
    if not meaningful_words: return []
    # Simple heuristic: Boost proper nouns (capitalized words, not at sentence start - approximation)
    proper_nouns = re.findall(r'(?<!\.\s)\b[A-Z][a-z]{3,}\b', text)
    proper_counts = Counter(p.lower() for p in proper_nouns if p.lower() not in stopwords)
    word_counts = Counter(meaningful_words)
    # Combine counts, giving slight boost to proper nouns maybe? Or just use frequency.
    # For now, stick to frequency of meaningful words.
    return [word for word, count in word_counts.most_common(top_n)]

# --- 6. AI Processing Placeholders / Simulation ---
# !!! Returns PRE-GENERATED EXAMPLE TEXT for April 4th, 2025 market crash !!!
def get_simulated_ai_output(topic, text_pool_len=0):
    # ... (Keep simulation logic and text from v4.2) ...
    logging.info(f"SIMULATING AI Synthesis/Analysis for: {topic} (Input pool size: {text_pool_len})")
    is_synthesis_topic = topic in ['debt_news','negative_news','ma_deals','tariff_news','commentary']
    # Add check for empty pool even for non-synthesis topics where relevant
    if text_pool_len == 0 and is_synthesis_topic:
         logging.warning(f"No text pool provided for {topic} synthesis, returning placeholder.")
         # Return a specific failure message if pool is empty
         return f"Data gathering for '{topic}' failed; synthesis skipped."
    # Pre-written text reflecting April 4th, 2025 market crash (Keep the text from v4.1/v4.2)
    outputs = {
        'executive_summary': "Markets extended their precipitous decline Friday, driven by intensifying global trade war fears following US tariff announcements and Chinese retaliation. The Dow plummeted over 2,200 points, the Nasdaq entered a bear market, and major indices suffered their worst week since March 2020 amid widespread panic and a flight from risk assets.",
        'debt_news': "Concerns intensified in credit markets as high-yield spreads remained elevated following Thursday's surge. While major defaults haven't materialized, the risk-off sentiment fueled by trade war fears is making investors reassess credit risk across the board, potentially impacting future debt issuance and refinancing, especially for companies reliant on global supply chains.",
        'negative_news': "The primary driver was the escalating US-China trade conflict, sparking fears of a damaging global trade war, significant economic slowdown, and potential stagflation (due to tariffs raising prices while slowing growth). The sheer scale and speed of the announced tariffs created extreme uncertainty. Geopolitical tensions and Fed comments on tariff-driven inflation further soured sentiment.",
        'ma_deals': "M&A news was overshadowed by the market turmoil. While underlying deal activity may continue, analysts suggest the heightened economic uncertainty and volatile valuations could pause larger, cross-border transactions until the trade situation clarifies. No major deals specific to April 4th were prominent in general market reports.",
        'tariff_news': "Following the US announcement of broad tariffs, China swiftly retaliated with its own list of duties, confirming market fears of a tit-for-tat escalation. Focus is now on the specific implementation details from both sides and potential reactions from other major trading partners like the EU. The lack of clear off-ramps intensified concerns.",
        'commentary': "Commentary described the market action as 'panic selling' and 'shellshock,' reflecting deep investor anxiety. Analysts highlighted the Nasdaq's entry into a bear market and the Dow's correction, expressing concerns the S&P 500 would follow. The outlook remains overwhelmingly negative, dominated by trade war uncertainty and expectations of continued high volatility.",
        'market_mood': "Extremely Negative / Fearful", # Based on context
        'why_it_matters_trade': "Significance: Heightened trade tensions threaten global growth, disrupt supply chains, and could lead to higher consumer prices, impacting corporate profits and economic stability.",
        'why_it_matters_risk': "Significance: The combination of trade fears and potential slowdown raises recession risks, forcing investors to re-evaluate asset allocations and potentially triggering further market instability.",
        'why_it_matters_debt': "Significance: Stressed credit markets can restrict companies' access to capital, hindering investment and growth, and potentially signaling broader financial fragility.",
        'what_to_watch': [ # Make this a list
            "Specifics of China's retaliatory tariffs and any further US responses.",
            "Statements from global policymakers (Fed, ECB, G7) addressing trade concerns.",
            "Upcoming economic data (esp. inflation, manufacturing PMIs) for signs of tariff impact."
        ]
    }
    output = outputs.get(topic, f"Placeholder simulation needed for {topic}")
    # Ensure failure message propagates if pool was empty for synthesis topics
    if text_pool_len == 0 and is_synthesis_topic:
        return f"Data gathering failed for '{topic}'; synthesis skipped."
    return output


def get_sentiment_analysis(items_list):
    # ... (Keep v4.2 logic) ...
    if not items_list: return "N/A"
    if not CONFIG['ai_processing'].get('sentiment_model'): return "N/A"
    logging.info(f"Performing sentiment analysis on {len(items_list)} items...")
    try:
        model_name = CONFIG['ai_processing']['sentiment_model']
        sentiment_pipeline = pipeline("sentiment-analysis", model=model_name, device=-1)
        texts_to_analyze = [item['text'][:512] for item in items_list[:30] if isinstance(item, dict) and item.get('text')]
        if not texts_to_analyze: return "No text for analysis."
        results = sentiment_pipeline(texts_to_analyze)
        positive_count = sum(1 for r in results if r['label'] == 'POSITIVE'); negative_count = sum(1 for r in results if r['label'] == 'NEGATIVE')
        logging.info(f"Sentiment Raw Counts: Pos={positive_count}, Neg={negative_count}")
        if negative_count == 0 and positive_count == 0: return "Neutral / No Sentiment"
        if negative_count > positive_count * 1.8: return "Strongly Negative"
        if negative_count > positive_count: return "Negative"
        if positive_count > negative_count * 1.8: return "Strongly Positive"
        if positive_count > negative_count: return "Positive"
        return "Mixed"
    except Exception as e: logging.error(f"Sentiment analysis failed: {e}", exc_info=True); return "Error Analyzing"

# --- 7. Data Fetching Orchestration (with User Input Integration) ---
# ** Define v5 version **
def get_data_pool_for_topic_v5(topic_key, user_input):
    """Gathers data pool including user input, API, RSS, scraping, secondary search."""
    logging.info(f"--- v5 Gathering data pool for topic: {topic_key} ---")
    text_pool = [] # List of text strings for AI
    display_items = [] # List of dicts {title, url, text, source, resolved_url} for display
    processed_urls = set()
    successful_scrapes = 0
    cfg_news = CONFIG['news_gathering']
    user_content_processed = False # Flag if user input led to data

    # --- Layer 0: User Input ---
    source_prefix_user = f"User Input:"
    # Add user raw text first
    if user_input.get('text'):
        logging.info(f"Processing user-provided text for {topic_key}")
        text_pool.append(f"{source_prefix_user} Direct Text\nContent: {user_input['text'][:5000]}")
        display_items.append({'title': 'User Provided Text Snippet', 'source': 'User Input', 'text': user_input['text'][:200]+'...', 'url': None, 'resolved_url': None})
        user_content_processed = True

    # Try scraping user URLs
    user_urls = user_input.get('urls', [])
    if user_urls and cfg_news['newspaper3k_enabled']:
        logging.info(f"Attempting to scrape {len(user_urls)} user-provided URLs for {topic_key}...")
        user_scraped_count = 0
        for url in user_urls:
            # Limit total scrapes per topic, even from user input? Decide later.
            # if successful_scrapes >= cfg_news['max_articles_to_scrape_per_topic']: break
            if url and url not in processed_urls:
                resolved_url = resolve_redirect(url)
                if resolved_url and resolved_url not in processed_urls:
                    scraped_data = scrape_article_text_newspaper(resolved_url)
                    if scraped_data:
                        logging.info(f"Success scraping user URL: {resolved_url}")
                        text_pool.append(f"{source_prefix_user} Scraped URL ({scraped_data['source']})\nTitle: {scraped_data['title']}\nContent: {scraped_data['scraped_text']}")
                        display_items.append({'title': scraped_data['title'], 'url': url, 'resolved_url': resolved_url, 'source': f"{scraped_data['source']} (User URL)", 'text': scraped_data['scraped_text'][:200]+'...'})
                        processed_urls.add(url); processed_urls.add(resolved_url)
                        successful_scrapes += 1
                        user_scraped_count += 1
                    else:
                         # Add placeholder even if scrape failed, to show attempt
                         display_items.append({'title': f"Failed scrape user URL", 'url': url, 'resolved_url': resolved_url, 'source': 'User Input', 'text': '[Scrape Failed]'})
                    time.sleep(CONFIG['polite_delay']) # Still delay between user URL scrapes
        if user_scraped_count > 0: user_content_processed = True


    # --- Automated Layers (Run if no sufficient user input OR to supplement) ---
    run_automated = True
    # Example condition: Run automated if user provided nothing or less than 2 successful items
    if user_content_processed and len(text_pool) >= 2:
        logging.info(f"Sufficient user content processed for {topic_key} ({len(text_pool)} items). Skipping some automated fetching.")
        run_automated = False # Adjust this logic as needed

    if run_automated:
        logging.info(f"Proceeding with automated fetching for {topic_key}...")
        max_ai_input_items_auto = cfg_news['max_articles_to_process_per_topic'] * 2 # Fetch more auto items if needed

        # Layer 1: NewsAPI (Conceptual)
        # ...

        # Layer 3: Primary RSS Feeds
        primary_rss_items_auto = []
        rss_feeds = cfg_news['rss_feeds'].get(topic_key, [])
        for feed_url in rss_feeds:
            source_name = f"RSS-{urlparse(feed_url).netloc}"
            rss_results = parse_rss_feed(feed_url, source_name=source_name, limit=cfg_news['max_items_per_primary_source'])
            if rss_results:
                 for item in rss_results:
                      if item.get('url') not in processed_urls and len(text_pool) < max_ai_input_items_auto:
                           text_pool.append(f"Source: {item['source']}\nTitle: {item['title']}\nContent: {item['text'][:1000]}")
                           display_items.append(item) # Add for display/scraping list
                           primary_rss_items_auto.append(item)
                           processed_urls.add(item['url'])
                           if item.get('resolved_url'): processed_urls.add(item.get('resolved_url'))
            time.sleep(CONFIG['polite_delay'])

        # Layer 4: Secondary "Hunt & Seek" Searches
        if primary_rss_items_auto:
            all_primary_text = " ".join([item['text'] for item in primary_rss_items_auto if item.get('text')])
            keywords = extract_keywords(all_primary_text, top_n=cfg_news['max_secondary_search_terms'])
            logging.info(f"Top keywords for secondary search ({topic_key}): {keywords}")
            for term in keywords:
                 secondary_search_url = f"https://news.google.com/rss/search?q={requests.utils.quote(term)}+when:1d&hl=en-US&gl=US&ceid=US:en"
                 secondary_results = parse_rss_feed(secondary_search_url, source_name=f"Search-{term}", limit=cfg_news['max_items_per_secondary_search'])
                 if secondary_results:
                      for item in secondary_results:
                           if item.get('url') not in processed_urls and len(text_pool) < max_ai_input_items_auto:
                                text_pool.append(f"Source: {item['source']}\nTitle: {item['title']}\nContent: {item['text'][:1000]}")
                                display_items.append(item)
                                primary_rss_items_auto.append(item) # Add to potential scrape list too
                                processed_urls.add(item['url'])
                                if item.get('resolved_url'): processed_urls.add(item.get('resolved_url'))
                 time.sleep(CONFIG['polite_delay'])

        # Layer 5: Article Scraping (newspaper3k) on gathered URLs
        if cfg_news['newspaper3k_enabled']:
             urls_to_attempt_scrape = list(dict.fromkeys([item.get('resolved_url') for item in primary_rss_items_auto if item.get('resolved_url')]))
             logging.info(f"Attempting automated newspaper3k scraping on up to {len(urls_to_attempt_scrape)} resolved URLs for {topic_key}...")
             current_scrapes_needed = max(0, cfg_news['max_articles_to_scrape_per_topic'] - successful_scrapes)

             for url in urls_to_attempt_scrape:
                 if successful_scrapes >= cfg_news['max_articles_to_scrape_per_topic']: break
                 if url and url not in processed_urls:
                      scraped_data = scrape_article_text_newspaper(url)
                      if scraped_data:
                           text_pool.append(f"Source: {scraped_data['source']} (Scraped)\nTitle: {scraped_data['title']}\nContent: {scraped_data['scraped_text']}")
                           # Add/Update display item
                           updated = False
                           for disp_item in display_items:
                                if disp_item.get('resolved_url') == url:
                                    disp_item.update({'title': scraped_data['title'], 'source': f"{scraped_data['source']} (Scraped)", 'text': scraped_data['scraped_text'][:200]+'...'})
                                    updated = True; break
                           if not updated: display_items.append({'title': scraped_data['title'], 'url': url, 'resolved_url': url, 'source': f"{scraped_data['source']} (Scraped)", 'text': scraped_data['scraped_text'][:200]+'...'})
                           processed_urls.add(url); successful_scrapes += 1
                      time.sleep(CONFIG['polite_delay'])

    logging.info(f"Final pool size for {topic_key}: {len(text_pool)} text pieces. User input used: {user_content_processed}")
    # Return the text pool AND the list of display items AND user input flag
    return text_pool if text_pool else [], display_items if display_items else [], user_content_processed


# --- Define other data fetching functions (Indices, Movers, Spotlight) ---
# These need to be defined *before* the main block calls them.
# ... (Paste the function definitions for get_indices_data, get_movers_data, get_data_spotlight from above) ...
def fetch_yfinance_indices(symbols): # Copied from above
    logging.info("Attempting yfinance for Indices..."); data = {}; all_failed = True
    for symbol in symbols:
        logging.debug(f"Fetching yfinance info for {symbol}")
        try:
            ticker = yf.Ticker(symbol); info = ticker.info; price = info.get('regularMarketPrice', info.get('currentPrice'))
            prev_close = info.get('previousClose', info.get('regularMarketPreviousClose')); change = info.get('regularMarketChange'); change_percent = info.get('regularMarketChangePercent')
            if price is not None and prev_close is not None and (change is None or change_percent is None):
                 if prev_close != 0: change = price - prev_close; change_percent = change / prev_close
                 else: change = None; change_percent = None
            if price is not None and prev_close is not None:
                data[symbol] = {'shortName': info.get('shortName', symbol), 'regularMarketPrice': price, 'regularMarketChange': change, 'regularMarketChangePercent': change_percent, 'previousClose': prev_close, 'source': 'yfinance Ticker.info'}
                all_failed = False; logging.info(f"Success fetching yfinance info for {symbol}")
            else: logging.warning(f"Incomplete data from yfinance .info for {symbol}."); data[symbol] = "Placeholder"
        except Exception as e: logging.error(f"yfinance Ticker failed for {symbol}: {e}", exc_info=True); data[symbol] = "Placeholder"
        time.sleep(0.1)
    return data if not all_failed else None

def get_indices_data(): # Copied from above
    logging.info("--- Getting Headline Indices ---"); data = None
    data = fetch_yfinance_indices(CONFIG['indices']['symbols'])
    if data: logging.info("Success using yfinance for indices."); return data
    placeholders = {symbol: "Placeholder" for symbol in CONFIG['indices']['symbols']}; logging.warning("All index methods failed. Using placeholders.")
    return placeholders

def get_movers_data(): # Copied from above
    logging.info("--- Getting Stock Movers ---")
    if not CONFIG['movers']['enabled']: logging.info("Movers fetching disabled."); return "Placeholder"
    logging.warning("!!! Movers scraping relies on CORRECT selectors in CONFIG - please verify them on live Yahoo Finance pages !!!")
    movers_config = CONFIG['movers']; selectors = movers_config['scrape_selectors']; limit = movers_config['limit']
    movers = {'gainers': [], 'losers': []}; success_flag = False
    # Scrape Gainers
    url_g = movers_config['scrape_gainers_url']; logging.info(f"Attempting scrape: {url_g}")
    response_g = make_request(url_g)
    if response_g:
        try:
            soup = BeautifulSoup(response_g.text, 'html.parser'); rows = soup.select(selectors['rows'])
            if not rows and selectors.get('fallback_rows'): rows = soup.select(selectors['fallback_rows'])
            if not rows: logging.warning(f"Movers: Could not find table rows on {url_g}")
            count = 0
            for i, row in enumerate(rows):
                if count >= limit: break;
                try:
                    cells = row.find_all('td', recursive=False);
                    if len(cells) > max(selectors['symbol_cell_index'], selectors['name_cell_index'], selectors['change_percent_cell_index']):
                        symbol_tag = cells[selectors['symbol_cell_index']].select_one(selectors['symbol_tag_selector']); name_tag_text = cells[selectors['name_cell_index']].get_text(strip=True)
                        change_percent_tag = cells[selectors['change_percent_cell_index']].select_one(selectors['change_percent_tag_selector']); symbol = symbol_tag.text.strip() if symbol_tag else None
                        name = name_tag_text if name_tag_text else None; change_pct_val = None
                        if change_percent_tag:
                            change_pct_val_attr = change_percent_tag.get('value'); change_pct_val_text = change_percent_tag.get_text(strip=True).replace('%','').replace('+','')
                            try: change_pct_val = float(change_pct_val_attr) * 100
                            except (ValueError, TypeError):
                                try: change_pct_val = float(change_pct_val_text)
                                except (ValueError, TypeError): change_pct_val = None
                        if symbol and name and change_pct_val is not None: movers['gainers'].append({'symbol': symbol, 'name': name, 'change_percent': f"{change_pct_val:.2f}" }); count += 1; success_flag = True
                        else: logging.debug(f"Gainers: Missing data row {i}")
                    else: logging.debug(f"Gainers: Not enough cells row {i}")
                except Exception as e_row: logging.error(f"Error parsing gainers row {i}: {e_row}", exc_info=False)
        except Exception as e: logging.error(f"Failed parsing gainers page {url_g}: {e}", exc_info=True)
    else: logging.warning(f"Failed to fetch gainers page: {url_g}")
    time.sleep(CONFIG['polite_delay'])
    # Scrape Losers (Similar logic)
    url_l = movers_config['scrape_losers_url']; logging.info(f"Attempting scrape: {url_l}")
    response_l = make_request(url_l)
    if response_l: # ... (Full Losers scraping logic similar to Gainers) ...
         try:
            soup = BeautifulSoup(response_l.text, 'html.parser'); rows = soup.select(selectors['rows'])
            if not rows and selectors.get('fallback_rows'): rows = soup.select(selectors['fallback_rows'])
            if not rows: logging.warning(f"Movers: Could not find table rows on {url_l}")
            count = 0
            for i, row in enumerate(rows):
                if count >= limit: break;
                try:
                    cells = row.find_all('td', recursive=False);
                    if len(cells) > max(selectors['symbol_cell_index'], selectors['name_cell_index'], selectors['change_percent_cell_index']):
                        symbol_tag = cells[selectors['symbol_cell_index']].select_one(selectors['symbol_tag_selector']); name_tag_text = cells[selectors['name_cell_index']].get_text(strip=True)
                        change_percent_tag = cells[selectors['change_percent_cell_index']].select_one(selectors['change_percent_tag_selector']); symbol = symbol_tag.text.strip() if symbol_tag else None
                        name = name_tag_text if name_tag_text else None; change_pct_val = None
                        if change_percent_tag:
                            change_pct_val_attr = change_percent_tag.get('value'); change_pct_val_text = change_percent_tag.get_text(strip=True).replace('%','').replace('+','')
                            try: change_pct_val = float(change_pct_val_attr) * 100
                            except (ValueError, TypeError):
                                try: change_pct_val = float(change_pct_val_text)
                                except (ValueError, TypeError): change_pct_val = None
                        if symbol and name and change_pct_val is not None: movers['losers'].append({'symbol': symbol, 'name': name, 'change_percent': f"{change_pct_val:.2f}" }); count += 1; success_flag = True
                        else: logging.debug(f"Losers: Missing data row {i}")
                    else: logging.debug(f"Losers: Not enough cells row {i}")
                except Exception as e_row: logging.error(f"Error parsing losers row {i}: {e_row}", exc_info=False)
         except Exception as e: logging.error(f"Failed parsing losers page {url_l}: {e}", exc_info=True)
    else: logging.warning(f"Failed to fetch losers page: {url_l}")

    if not success_flag: logging.warning("Movers scraping failed completely."); return "Placeholder"
    logging.info(f"Movers data fetched (Gainers: {len(movers['gainers'])}, Losers: {len(movers['losers'])}).")
    return movers

def get_data_spotlight(): # Copied from above
     logging.info("--- Getting Data Spotlight ---"); symbol = CONFIG['data_spotlight']['symbol']; name = CONFIG['data_spotlight']['name']
     try:
          ticker = yf.Ticker(symbol); info = ticker.info; price = info.get('regularMarketPrice', info.get('currentPrice')); change = info.get('regularMarketChange'); change_pct = info.get('regularMarketChangePercent')
          if price is not None and change is not None and change_pct is not None:
               logging.info(f"Success fetching spotlight {symbol} via .info")
               return {'name': name, 'symbol': symbol, 'value': f"{price:.2f}", 'change': f"{change:+.2f} ({change_pct*100:+.2f}%)"}
          else:
               hist = ticker.history(period="2d");
               if not hist.empty and len(hist) >= 2:
                   latest_close = hist['Close'].iloc[-1]; prev_close = hist['Close'].iloc[-2]
                   if prev_close != 0: change = latest_close - prev_close; change_pct = change / prev_close
                   else: change = 0; change_pct = 0
                   logging.info(f"Success fetching spotlight {symbol} via history")
                   return {'name': name, 'symbol': symbol, 'value': f"{latest_close:.2f}", 'change': f"{change:+.2f} ({change_pct*100:+.2f}%)"}
               raise ValueError("Could not get spotlight data from .info or history")
     except Exception as e: logging.error(f"Failed spotlight {symbol}: {e}", exc_info=True)
     return {'name': name, 'symbol': symbol, 'value': 'N/A', 'change': 'N/A'}


# --- 9. Formatting Function (Ensure NEWSLETTER_TEMPLATE_V5 is defined) ---
# ... (Keep format_newsletter function using NEWSLETTER_TEMPLATE_V5) ...
def format_newsletter(data):
    logging.info("Formatting newsletter...")
    try:
        def truncate_filter(s, length=250, killwords=False, end='...'): # Ensure filter is defined
             if s is None: return '' ; s = str(s)
             if len(s) <= length: return s
             if killwords: return s[:length-len(end)] + end
             # Ensure splitting doesn't fail on short strings
             split_point = length - len(end) + 1
             if split_point <= 0: return end # Handle edge case
             words = s[:split_point].rsplit(' ', 1) # Split from right
             if len(words) > 1 : return words[0] + end
             else: return s[:length-len(end)] + end # Fallback if no space found
        env = Environment(); env.filters['truncate'] = truncate_filter
        # Make sure template string is accessible
        template = env.from_string(NEWSLETTER_TEMPLATE_V5)
        return template.render(data)
    except Exception as e:
        logging.error(f"Failed to render Jinja2 template: {e}", exc_info=True)
        return f"<html><body><h1>Template Rendering Error</h1><p>{e}</p></body></html>"

# --- 10. Main Generation and Interaction Logic ---

# Store initial data globally or pass through widget callbacks
global_newsletter_data = {}
global_data_pools = {}

def generate_newsletter_data(user_override_inputs=None):
    """Fetches all data and prepares the dictionary for the template."""
    logging.info(f"--- generate_newsletter_data called ---")
    run_datetime = datetime.now(timezone.utc)
    run_date_str = run_datetime.strftime('%Y-%m-%d')

    # Use override inputs if provided (from widgets), otherwise use initial form values
    current_user_inputs = user_override_inputs if user_override_inputs is not None else USER_INPUTS

    # Fetch Core Structured Data (Run every time)
    indices = get_indices_data()
    time.sleep(CONFIG['polite_delay'] / 2)
    movers = get_movers_data()
    time.sleep(CONFIG['polite_delay'] / 2)
    spotlight = get_data_spotlight()

    # Gather Raw Text Pools for Each Topic (incorporating current user input)
    topic_keys = ['debt', 'risk', 'deals', 'trade', 'commentary', 'general']
    data_pools = {} # Store raw display items {topic: [items]}
    text_pools = {} # Store text strings for AI {topic: [texts]}
    user_input_flags = defaultdict(bool)

    for key in topic_keys:
        topic_user_input = current_user_inputs.get(key, {'urls': [], 'text': ''})
        # Call function that handles user input + automated fetching
        pool_texts, pool_display_items, user_flag = get_data_pool_for_topic_v5(key, topic_user_input)
        text_pools[key] = pool_texts
        data_pools[key] = pool_display_items # Store display items separately
        user_input_flags[key] = user_flag
        time.sleep(CONFIG['polite_delay'])

    # Perform AI Processing (Simulated)
    ai_outputs = {}
    if CONFIG['ai_processing']['enabled']:
        logging.info("AI Processing Enabled (Using Simulated Outputs)")
        ai_outputs['debt_news'] = get_simulated_ai_output('debt_news', len(text_pools.get('debt', [])))
        ai_outputs['negative_news'] = get_simulated_ai_output('negative_news', len(text_pools.get('risk', [])))
        ai_outputs['ma_deals'] = get_simulated_ai_output('ma_deals', len(text_pools.get('deals', [])))
        ai_outputs['tariff_news'] = get_simulated_ai_output('tariff_news', len(text_pools.get('trade', [])))
        ai_outputs['commentary'] = get_simulated_ai_output('commentary', len(text_pools.get('commentary', [])))
        ai_outputs['executive_summary'] = get_simulated_ai_output('executive_summary')
        ai_outputs['what_to_watch'] = get_simulated_ai_output('what_to_watch')
        ai_outputs['why_matters_trade'] = get_simulated_ai_output('why_it_matters_trade')
        ai_outputs['why_matters_risk'] = get_simulated_ai_output('why_it_matters_risk')
        ai_outputs['why_matters_debt'] = get_simulated_ai_output('why_it_matters_debt')
        # Sentiment analysis
        all_display_items_for_sentiment = [item for pool in data_pools.values() for item in pool]
        ai_outputs['market_mood'] = get_sentiment_analysis(all_display_items_for_sentiment)
        if ai_outputs['market_mood'] in ["N/A", "Error Analyzing"]:
             ai_outputs['market_mood'] = get_simulated_ai_output('market_mood')
    else: # AI Disabled
        # ... (placeholder assignment) ...
        logging.warning("AI Processing Disabled.")
        for topic in ['debt_news', 'negative_news', 'ma_deals', 'tariff_news', 'commentary', 'executive_summary', 'what_to_watch', 'market_mood', 'why_matters_trade', 'why_matters_risk', 'why_matters_debt']:
             ai_outputs[topic] = f"Placeholder [{topic} - AI Disabled]"


    # Prepare Final Data Object for Template
    final_template_data = {
        'date_long': run_datetime.strftime('%B %d, %Y %H:%M:%S %Z'),
        'date_short': run_date_str,
        'newsletter_title': CONFIG['output']['newsletter_title'],
        'index_data': indices,
        'stock_movers': movers,
        'data_spotlight': spotlight,
        'sector_notes': CONFIG['output']['sector_notes_placeholder'],
        'data_pools': data_pools, # Pass raw display items for template
        'user_input_flags': user_input_flags,
        **ai_outputs
    }
    return final_template_data, data_pools # Return data pools separately if needed


# --- 11. Interactive UI Setup & Execution ---

# Define widgets (global scope needed for callback access)
widgets_dict = {}
section_keys = ['debt', 'risk', 'deals', 'trade', 'commentary']
for key in section_keys:
    widgets_dict[f'{key}_urls'] = widgets.Textarea(value='', placeholder=f'Paste URLs for {key.title()} section (one per line)', layout=widgets.Layout(height='80px', width='95%'))
    widgets_dict[f'{key}_text'] = widgets.Textarea(value='', placeholder=f'Paste text/summary for {key.title()} section', layout=widgets.Layout(height='100px', width='95%'))

update_button = widgets.Button(description="Update Newsletter with Inputs", button_style='info', icon='refresh')
initial_output_area = widgets.Output() # To display the first generated newsletter
update_output_area = widgets.Output() # To display the updated newsletter

# Function triggered by button click
def on_update_button_clicked(b):
    logging.info("--- Update button clicked ---")
    with update_output_area: # Capture output/errors in this area
        clear_output(wait=True) # Clear previous update results
        print("Processing user inputs and regenerating newsletter...")
        # Create user_override_inputs from widget values
        user_override_inputs = defaultdict(lambda: {'urls': [], 'text': ''})
        for key in section_keys:
             urls_text = widgets_dict[f'{key}_urls'].value
             text_val = widgets_dict[f'{key}_text'].value
             user_override_inputs[key]['urls'] = [url.strip() for url in urls_text.splitlines() if url.strip()]
             user_override_inputs[key]['text'] = text_val.strip()

        try:
            # Regenerate data using the *current* widget values
            updated_newsletter_data, _ = generate_newsletter_data(user_override_inputs=user_override_inputs) # Ignore returned pools for update display
            updated_html = format_newsletter(updated_newsletter_data)
            print("Update complete.")
            display(HTML(updated_html)) # Display the new HTML
             # Optionally save the updated version
            output_filename = f"Market_Mayhem_Updated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            with open(output_filename, 'w', encoding='utf-8') as f: f.write(updated_html)
            logging.info(f"Updated newsletter saved to {output_filename}")

        except Exception as e:
             logging.error("Error during newsletter update:", exc_info=True)
             print(f"An error occurred during the update: {e}")


update_button.on_click(on_update_button_clicked)

# Main Execution Flow
if __name__ == "__main__":
    logging.info(f"====== Starting {CONFIG['output']['newsletter_title']} v5.1 Initial Generation ======")
    start_time = time.time()

    # 1. Generate initial data (using initial USER_INPUTS from forms if any)
    initial_newsletter_data, initial_data_pools = generate_newsletter_data()

    # 2. Format and display initial newsletter
    initial_html = format_newsletter(initial_newsletter_data)
    with initial_output_area:
         display(HTML(initial_html))

         # Save initial version
         output_filename = f"Market_Mayhem_Initial_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
         try:
             with open(output_filename, 'w', encoding='utf-8') as f: f.write(initial_html)
             logging.info(f"Initial newsletter saved to: {output_filename}")
         except Exception as e_save:
             logging.error(f"Error saving initial newsletter file: {e_save}", exc_info=True)


    # 3. Display Interactive Widgets
    print("\n--- Provide Additional Input (Optional) ---")
    input_widgets = []
    for key in section_keys:
        input_widgets.append(widgets.Label(f"{key.replace('_',' ').title()}:"))
        input_widgets.append(widgets.HBox([widgets.Label("URLs:", layout=widgets.Layout(width='50px')), widgets_dict[f'{key}_urls']]))
        input_widgets.append(widgets.HBox([widgets.Label("Text:", layout=widgets.Layout(width='50px')), widgets_dict[f'{key}_text']]))

    display(widgets.VBox(input_widgets), update_button, update_output_area)


    end_time = time.time()
    logging.info(f"====== Initial Generation & UI Setup Complete ({end_time - start_time:.2f} seconds) ======")
