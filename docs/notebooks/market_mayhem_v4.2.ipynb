# -*- coding: utf-8 -*-
"""
Market Mayhem Newsletter Generator - v4.2 (Hunt & Seek v1, Stability)

- Fixes TypeError by returning [] from get_data_pool_for_topic.
- Updates Movers selectors (NEEDS VERIFICATION).
- Attempts to resolve redirects before newspaper3k scraping.
- Implements basic "Hunt & Seek" via secondary Google News searches.
- Refines RSS sources, enhances logging.
"""

# --- 1. Setup: Install Libraries ---
!pip install yfinance requests beautifulsoup4 Jinja2 feedparser newspaper3k transformers torch sentencepiece alpha_vantage newsapi-python lxml[html_clean] -q

# --- 2. Imports ---
import yfinance as yf
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone, timedelta
import time
import re
import os
import feedparser
from jinja2 import Environment, Template
from urllib.parse import urljoin, urlparse
import logging
from collections import defaultdict, Counter
try:
    from newspaper import Article, Config as NewspaperConfig
    NEWSPAPER_AVAILABLE = True
except ImportError as e_imp:
    logging.error(f"Newspaper3k import failed: {e_imp}. Article scraping will be disabled.")
    NEWSPAPER_AVAILABLE = False
import traceback

# Optional API Clients
# ... (TimeSeries, NewsApiClient imports)

# Transformers
from transformers import pipeline, logging as hf_logging
hf_logging.set_verbosity_error()

# --- 3. Logging Setup ---
for handler in logging.root.handlers[:]: logging.root.removeHandler(handler)
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - [%(funcName)s] %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')
logging.info("Libraries installed and imported. Logging configured.")

# --- 4. Configuration ---
# *** REVIEW & VERIFY MOVERS SELECTORS AND RSS FEEDS ***
CONFIG = {
    'request_headers': {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}, # Try Googlebot UA
    'request_timeout': 30, # Increased timeout slightly
    'polite_delay': 0.8, # Faster delay
    'target_date_str': datetime.now(timezone.utc).strftime('%Y-%m-%d'),

    'api_keys': { # Set in Colab Secrets
        'alpha_vantage': 'ALPHA_VANTAGE_API_KEY',
        'newsapi': 'NEWS_API_KEY'
    },

    'indices': {'symbols': ['^GSPC', '^DJI', '^IXIC']},
    'movers': {
        'enabled': True,
        'scrape_gainers_url': "https://finance.yahoo.com/gainers?count=25",
        'scrape_losers_url': "https://finance.yahoo.com/losers?count=25",
        'scrape_selectors': { # *** UPDATED GUESSES - MUST VERIFY ON LIVE YAHOO FINANCE ***
             'rows': 'table > tbody > tr', # Simpler selector, hope it targets the main table
             'fallback_rows': 'tr.simpTblRow', # Another possibility found sometimes
             'symbol_cell_index': 0,
             'name_cell_index': 1,
             'change_percent_cell_index': 4,
             'symbol_tag_selector': 'a[data-test="quoteLink"]',
             'change_percent_tag_selector': 'fin-streamer[data-field="regularMarketChangePercent"]' # Streamer tag often used now
        },
        'limit': 5
    },
    'data_spotlight': {'symbol': '^VIX', 'name': 'VIX (Fear Gauge)'},
    'news_gathering': {
        'rss_feeds': { # Use working feeds + Google News searches
            'debt': ["https://news.google.com/rss/search?q=corporate+debt+OR+bond+market+OR+interest+rates+when:1d&hl=en-US&gl=US&ceid=US:en"], # Primary source: Google News
                     # Add specific, verified bond news RSS feeds if found
            'risk': ["https://rss.nytimes.com/services/xml/rss/nyt/World.xml", "https://www.pbs.org/newshour/feeds/rss/headlines"],
            'deals': ["https://news.google.com/rss/search?q=M%26A+OR+acquisition+OR+merger+when:1d&hl=en-US&gl=US&ceid=US:en"],
            'trade': ["https://news.google.com/rss/search?q=trade+OR+tariff+OR+commerce+department+when:1d&hl=en-US&gl=US&ceid=US:en"],
            'commentary': ["https://feeds.a.dj.com/rss/RSSMarketsMain.xml", "https://www.investing.com/rss/news_25.rss"],
            'general': ["https://rss.nytimes.com/services/xml/rss/nyt/Business.xml", "https://feeds.skynews.com/feeds/rss/business.xml", "http://feeds.bbci.co.uk/news/business/rss.xml"] # Added BBC Business
        },
        'newspaper3k_enabled': NEWSPAPER_AVAILABLE,
        'max_items_per_primary_source': 7, # Max items from each initial RSS/API source
        'max_secondary_search_terms': 2, # How many top terms to trigger secondary searches for
        'max_items_per_secondary_search': 3, # Max items from each secondary search
        'max_articles_to_scrape_per_topic': 5, # Max successful newspaper scrapes
        'min_article_text_length': 300, # Minimum length for scraped text to be considered valid
        'keyword_extraction_stopwords': ['market', 'stock', 'shares', 'inc', 'corp', 'company', 'says', 'report', 'news', 'update', 'daily', 'investor', 'trading'] # Simple stopwords
    },
    'ai_processing': { # Settings for (simulated) AI
        'enabled': True,
        'sentiment_model': 'distilbert-base-uncased-finetuned-sst-2-english',
    },
     'output': {
         'newsletter_title': 'Market Mayhem - Daily Briefing v4.2',
         'sector_notes_placeholder': 'Sector performance analysis pending.' # Generic placeholder
     }
}

# --- 5. Helper Functions (URL resolving, keyword extraction added) ---

def make_request(url, retries=1, delay=1, stream=False, allow_redirects=True):
    """Fetches URL content with better error handling and optional streaming."""
    logging.debug(f"Requesting URL: {url}")
    for attempt in range(retries + 1):
        try:
            response = requests.get(url, headers=CONFIG['request_headers'], timeout=CONFIG['request_timeout'], stream=stream, allow_redirects=allow_redirects)
            logging.debug(f"Attempt {attempt+1}: Status Code {response.status_code} for {url}")
            # Check if status code indicates success *before* raise_for_status
            if response.status_code >= 400:
                 # Log client/server errors as warnings unless it's the last attempt
                 level = logging.ERROR if attempt == retries else logging.WARNING
                 logging.log(level, f"Attempt {attempt+1}: HTTP {response.status_code} error fetching {url}")
                 # Specific handling for common errors
                 if response.status_code in [403, 401, 404]: break # Don't retry these
                 if response.status_code == 429: # Rate limited
                     wait_time = delay * 5 # Longer wait for rate limits
                     logging.warning(f"Rate limited (429). Waiting {wait_time}s before retry...")
                     time.sleep(wait_time)
                     continue # Go to next attempt
                 # For other 4xx/5xx errors, proceed to retry logic if applicable
            else:
                 # Success!
                 return response # Return successful response

        except requests.exceptions.Timeout:
            logging.warning(f"Attempt {attempt+1}: Timeout error fetching {url}")
        except requests.exceptions.TooManyRedirects:
             logging.warning(f"Attempt {attempt+1}: Too many redirects fetching {url}")
             break # Don't retry redirect loops
        except requests.exceptions.RequestException as e:
            logging.warning(f"Attempt {attempt+1}: Request exception for {url}: {e}")
            # Break on certain connection errors? Depends on error type.
        except Exception as e:
            logging.error(f"Attempt {attempt+1}: Unexpected request error for {url}: {e}", exc_info=True)
            break # Don't retry on unexpected errors

        # Retry logic
        if attempt < retries:
            wait_time = delay * (2 ** attempt)
            logging.info(f"Retrying {url} in {wait_time:.1f} seconds...")
            time.sleep(wait_time)

    logging.error(f"Failed to fetch URL {url} after {retries+1} attempts.")
    return None

def resolve_redirect(url):
    """Attempts to resolve redirects to find the final article URL."""
    if "news.google.com/rss/articles/" not in url:
        return url # Not a Google News redirect link

    logging.debug(f"Attempting to resolve redirect for: {url}")
    try:
        # Use HEAD request first to be faster and less resource-intensive
        response = requests.head(url, headers=CONFIG['request_headers'], allow_redirects=True, timeout=15)
        if 200 <= response.status_code < 400: # Check if final status is okay
            final_url = response.url
            logging.debug(f"Resolved redirect: {url} -> {final_url}")
            return final_url
        else:
            logging.warning(f"HEAD request failed for redirect resolution ({response.status_code}): {url}")
            # Fallback: Try GET request carefully
            response_get = make_request(url, retries=0, allow_redirects=True) # No retries on fallback GET
            if response_get:
                 final_url_get = response_get.url
                 logging.debug(f"Resolved redirect via GET: {url} -> {final_url_get}")
                 return final_url_get
            else:
                 logging.warning(f"GET request also failed for redirect resolution: {url}")
                 return url # Return original if resolution fails
    except Exception as e:
        logging.warning(f"Error resolving redirect for {url}: {e}")
        return url # Return original on error

def parse_rss_feed(feed_url, source_name, limit=7):
    """Fetches and parses RSS, returns list of dicts {title, url, source, text, resolved_url}."""
    logging.info(f"Attempting RSS: {source_name} ({feed_url})")
    items_for_ai = []
    try:
        response = make_request(feed_url)
        if not response: return None

        feed = feedparser.parse(response.content)
        # Relaxed bozo check - proceed even if minor issues occur
        if feed.bozo: logging.warning(f"Feedparser issue parsing {feed_url}: {feed.bozo_exception}")
        if not feed.entries: logging.warning(f"No entries found in RSS feed: {feed_url}"); return None

        for entry in feed.entries[:limit]:
            title = getattr(entry, 'title', None)
            original_link = getattr(entry, 'link', None)
            if not title or not original_link: continue # Skip entries without title or link

            # Extract text content
            content_html = ''
            if hasattr(entry, 'content') and entry.content: content_html = entry.content[0].value
            if not content_html and hasattr(entry, 'summary'): content_html = entry.summary
            if not content_html and hasattr(entry, 'description'): content_html = entry.description
            text_content = BeautifulSoup(content_html, "html.parser").get_text(separator=' ', strip=True) if content_html else None
            final_text = text_content if text_content and len(text_content) > 30 else title

            if len(title) > 10: # Basic filter
                 # Attempt to resolve redirect immediately for later scraping
                 resolved_url = resolve_redirect(original_link)
                 time.sleep(0.2) # Small delay after resolving
                 items_for_ai.append({'title': title, 'url': original_link, 'resolved_url': resolved_url, 'source': source_name, 'text': final_text})

        logging.info(f"Parsed {len(items_for_ai)} relevant items from RSS: {source_name}")
        return items_for_ai if items_for_ai else None
    except Exception as e:
        logging.error(f"Failed to process RSS feed {feed_url}: {e}", exc_info=True)
    return None

def scrape_article_text_newspaper(url_to_scrape, original_url=""):
    """Uses newspaper3k if available, operates on resolved URL."""
    if not CONFIG['news_gathering']['newspaper3k_enabled'] or not url_to_scrape: return None
    logging.info(f"Attempting Newspaper3k scrape on resolved URL: {url_to_scrape} (Original: {original_url or 'N/A'})")
    try:
        parsed_link = urlparse(url_to_scrape)
        if not parsed_link.scheme.startswith('http'):
             logging.warning(f"Skipping newspaper scrape for non-http resolved URL: {url_to_scrape}")
             return None

        news_config = NewspaperConfig()
        news_config.browser_user_agent = CONFIG['request_headers']['User-Agent']
        news_config.request_timeout = CONFIG['request_timeout']
        news_config.fetch_images = False
        news_config.memoize_articles = False
        news_config.verbose = False

        article = Article(url_to_scrape, config=news_config)
        article.download()
        article.parse()

        min_len = CONFIG['news_gathering']['min_article_text_length']
        if article.text and len(article.text) >= min_len:
            logging.info(f"Newspaper3k SUCCESS for: {url_to_scrape}")
            title = article.title if article.title else "Title N/A"
            source_netloc = parsed_link.netloc.replace('www.', '')
            # Return structured data instead of just text
            return {'source': source_netloc, 'title': title, 'scraped_text': article.text[:4000]} # Limit length
        else:
             logging.warning(f"Newspaper3k: Text length ({len(article.text) if article.text else 0}) < {min_len} or parse failed for {url_to_scrape}")
             return None
    except Exception as e:
        logging.warning(f"Newspaper3k failed for {url_to_scrape}: {e}")
        return None

def extract_keywords(text, top_n=3):
    """Simple keyword extraction based on word frequency (excluding stopwords)."""
    if not text: return []
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower()) # Find words >= 4 chars
    stopwords = set(CONFIG['news_gathering']['keyword_extraction_stopwords'])
    meaningful_words = [word for word in words if word not in stopwords]
    if not meaningful_words: return []
    word_counts = Counter(meaningful_words)
    return [word for word, count in word_counts.most_common(top_n)]


# --- 6. AI Processing Placeholders (Unchanged) ---
def get_simulated_ai_output(topic, text_pool_len=0):
    # (Same simulation logic as v4.1, using pre-written text)
    # ... (keep the pre-written April 4th text) ...
    logging.info(f"SIMULATING AI Synthesis/Analysis for: {topic} (Input pool size: {text_pool_len})")
    # Add check for empty pool
    if text_pool_len == 0 and topic not in ['executive_summary', 'market_mood', 'what_to_watch', 'why_it_matters_trade', 'why_it_matters_risk', 'why_it_matters_debt']:
         logging.warning(f"No text pool provided for {topic} synthesis, returning placeholder.")
         return f"Data gathering failed for {topic}. Cannot synthesize."
    # Pre-written text reflecting April 4th, 2025 market crash (Keep the text from v4.1)
    outputs = {
        'executive_summary': "Markets extended their precipitous decline Friday, driven by intensifying global trade war fears following US tariff announcements and Chinese retaliation. The Dow plummeted over 2,200 points, the Nasdaq entered a bear market, and major indices suffered their worst week since March 2020 amid widespread panic and a flight from risk assets.",
        'debt_news': "Concerns intensified in credit markets as high-yield spreads remained elevated following Thursday's surge. While major defaults haven't materialized, the risk-off sentiment fueled by trade war fears is making investors reassess credit risk across the board, potentially impacting future debt issuance and refinancing, especially for companies reliant on global supply chains.",
        'negative_news': "The primary driver was the escalating US-China trade conflict, sparking fears of a damaging global trade war, significant economic slowdown, and potential stagflation (due to tariffs raising prices while slowing growth). The sheer scale and speed of the announced tariffs created extreme uncertainty. Geopolitical tensions and Fed comments on tariff-driven inflation further soured sentiment.",
        'ma_deals': "M&A news was overshadowed by the market turmoil. While underlying deal activity may continue, analysts suggest the heightened economic uncertainty and volatile valuations could pause larger, cross-border transactions until the trade situation clarifies. No major deals specific to April 4th were prominent in general market reports.",
        'tariff_news': "Following the US announcement of broad tariffs, China swiftly retaliated with its own list of duties, confirming market fears of a tit-for-tat escalation. Focus is now on the specific implementation details from both sides and potential reactions from other major trading partners like the EU. The lack of clear off-ramps intensified concerns.",
        'commentary': "Commentary described the market action as 'panic selling' and 'shellshock,' reflecting deep investor anxiety. Analysts highlighted the Nasdaq's entry into a bear market and the Dow's correction, expressing concerns the S&P 500 would follow. The outlook remains overwhelmingly negative, dominated by trade war uncertainty and expectations of continued high volatility.",
        'market_mood': "Extremely Negative / Fearful", # Based on context
        'why_it_matters_trade': "Significance: Heightened trade tensions threaten global growth, disrupt supply chains, and could lead to higher consumer prices, impacting corporate profits and economic stability.",
        'why_it_matters_risk': "Significance: The combination of trade fears and potential slowdown raises recession risks, forcing investors to re-evaluate asset allocations and potentially triggering further market instability.",
        'why_it_matters_debt': "Significance: Stressed credit markets can restrict companies' access to capital, hindering investment and growth, and potentially signaling broader financial fragility.",
        'what_to_watch': [
            "Specifics of China's retaliatory tariffs and any further US responses.",
            "Statements from global policymakers (Fed, ECB, G7) addressing trade concerns.",
            "Upcoming economic data (esp. inflation, manufacturing PMIs) for signs of tariff impact."
        ]
    }
    output = outputs.get(topic, f"Placeholder simulation needed for {topic}")
    if text_pool_len == 0 and topic in ['debt_news','negative_news','ma_deals','tariff_news','commentary']:
        return f"Data gathering failed for {topic}, cannot provide synthesis."
    return output


def get_sentiment_analysis(items_list):
    # (Same as v4.1, but ensure it takes list of dicts)
    if not items_list: return "N/A"
    if not CONFIG['ai_processing'].get('sentiment_model'): return "N/A"
    logging.info(f"Performing sentiment analysis on {len(items_list)} items...")
    try:
        model_name = CONFIG['ai_processing']['sentiment_model']
        sentiment_pipeline = pipeline("sentiment-analysis", model=model_name, device=-1)
        # Use the 'text' field which should contain summary/title
        texts_to_analyze = [item['text'][:512] for item in items_list[:30] if isinstance(item, dict) and item.get('text')]
        if not texts_to_analyze: return "No text for analysis."

        results = sentiment_pipeline(texts_to_analyze)
        # ... (rest of sentiment calculation logic from v4.1) ...
        positive_count = sum(1 for r in results if r['label'] == 'POSITIVE')
        negative_count = sum(1 for r in results if r['label'] == 'NEGATIVE')
        logging.info(f"Sentiment Raw Counts: Pos={positive_count}, Neg={negative_count}")
        if negative_count == 0 and positive_count == 0: return "Neutral / No Sentiment"
        if negative_count > positive_count * 2: return "Strongly Negative"
        if negative_count > positive_count: return "Negative"
        if positive_count > negative_count * 2: return "Strongly Positive"
        if positive_count > negative_count: return "Positive"
        return "Mixed"

    except Exception as e:
        logging.error(f"Sentiment analysis failed: {e}", exc_info=True)
        return "Error Analyzing"


# --- 7. Section-Specific Data Fetching (get_indices, get_movers, get_spotlight - refined) ---
# get_indices_data, get_data_spotlight (largely unchanged from v4.1 as they worked)
def get_indices_data():
     # ... (Keep v4.1 logic using yfinance info) ...
    logging.info("--- Getting Headline Indices ---")
    indices_config = CONFIG['indices']
    data = None
    # Layer 2: Try yfinance (using .info primarily)
    data = fetch_yfinance_indices(indices_config['symbols']) # Assuming fetch_yfinance_indices is defined as in v4.1
    if data: logging.info("Success using yfinance for indices."); return data
    # Layer 7: Placeholder
    placeholders = {symbol: "Placeholder" for symbol in indices_config['symbols']}
    logging.warning("All index methods failed. Using placeholders.")
    return placeholders


def get_data_spotlight():
     # ... (Keep v4.1 logic using yfinance info/history) ...
     logging.info("--- Getting Data Spotlight ---")
     symbol = CONFIG['data_spotlight']['symbol']
     name = CONFIG['data_spotlight']['name']
     try:
          ticker = yf.Ticker(symbol)
          info = ticker.info
          price = info.get('regularMarketPrice', info.get('currentPrice'))
          change = info.get('regularMarketChange')
          change_pct = info.get('regularMarketChangePercent')
          if price is not None and change is not None and change_pct is not None:
               logging.info(f"Success fetching spotlight data for {symbol} via .info")
               return {'name': name, 'symbol': symbol, 'value': f"{price:.2f}", 'change': f"{change:+.2f} ({change_pct*100:+.2f}%)"}
          else: # Try history fallback
               hist = ticker.history(period="2d")
               if not hist.empty and len(hist) >= 2:
                   latest_close = hist['Close'].iloc[-1]; prev_close = hist['Close'].iloc[-2]
                   if prev_close != 0:
                       change = latest_close - prev_close; change_pct = change / prev_close
                       logging.info(f"Success fetching spotlight data for {symbol} via history")
                       return {'name': name, 'symbol': symbol, 'value': f"{latest_close:.2f}", 'change': f"{change:+.2f} ({change_pct*100:+.2f}%)"}
               raise ValueError("Could not get spotlight data from .info or history")
     except Exception as e: logging.error(f"Failed to get data for spotlight {symbol}: {e}", exc_info=True)
     return {'name': name, 'symbol': symbol, 'value': 'N/A', 'change': 'N/A'}


def get_movers_data():
    # Uses updated CONFIG selectors (NEED VERIFICATION)
    logging.info("--- Getting Stock Movers ---")
    if not CONFIG['movers']['enabled']:
         logging.info("Movers fetching disabled in config.")
         return "Placeholder"
    # ... (Keep refined parsing logic from v4.1, but ensure selectors in CONFIG are updated/verified) ...
    movers_config = CONFIG['movers']
    selectors = movers_config['scrape_selectors']
    limit = movers_config['limit']
    movers = {'gainers': [], 'losers': []}

    # --- Scrape Gainers ---
    url_g = movers_config['scrape_gainers_url']
    logging.info(f"Attempting scrape: {url_g}")
    response_g = make_request(url_g)
    if response_g:
        try:
            soup = BeautifulSoup(response_g.text, 'html.parser')
            rows = soup.select(selectors['rows'])
            if not rows and selectors.get('fallback_rows'):
                 logging.warning(f"Primary selector '{selectors['rows']}' failed for gainers, trying fallback '{selectors['fallback_rows']}'")
                 rows = soup.select(selectors['fallback_rows'])
            if not rows: logging.warning(f"Movers: Could not find table rows on {url_g} with configured selectors.")

            count = 0
            for i, row in enumerate(rows):
                if count >= limit: break
                try:
                    cells = row.find_all('td', recursive=False) # Avoid nested table cells
                    if len(cells) > max(selectors['symbol_cell_index'], selectors['name_cell_index'], selectors['change_percent_cell_index']):
                        symbol_tag = cells[selectors['symbol_cell_index']].select_one(selectors['symbol_tag_selector'])
                        # Name might just be text in the cell
                        name_tag_text = cells[selectors['name_cell_index']].get_text(strip=True)
                        change_percent_tag = cells[selectors['change_percent_cell_index']].select_one(selectors['change_percent_tag_selector'])

                        # Check if tags were found
                        symbol = symbol_tag.text.strip() if symbol_tag else None
                        name = name_tag_text if name_tag_text else None
                        change_pct_val = None
                        if change_percent_tag:
                            # Check for value attribute or text content
                             change_pct_val_attr = change_percent_tag.get('value')
                             change_pct_val_text = change_percent_tag.get_text(strip=True).replace('%','').replace('+','')
                             # Prioritize value attribute if it looks like a number
                             try: change_pct_val = float(change_pct_val_attr) * 100 # Assume value is ratio
                             except (ValueError, TypeError):
                                 try: change_pct_val = float(change_pct_val_text)
                                 except (ValueError, TypeError): change_pct_val = None

                        if symbol and name and change_pct_val is not None:
                            movers['gainers'].append({'symbol': symbol, 'name': name, 'change_percent': f"{change_pct_val:.2f}" })
                            count += 1
                        else:
                             logging.debug(f"Gainers: Missing data in row {i}. Sym:{symbol is not None}, Name:{name is not None}, Change:{change_pct_val is not None}")
                    else: logging.debug(f"Gainers: Not enough cells ({len(cells)}) in row {i}")
                except Exception as e_row: logging.error(f"Error parsing gainers row {i}: {e_row}", exc_info=False) # Less verbose traceback for row errors

        except Exception as e: logging.error(f"Failed parsing gainers page {url_g}: {e}", exc_info=True)
    else: logging.warning(f"Failed to fetch gainers page: {url_g}")
    time.sleep(CONFIG['polite_delay'])

    # --- Scrape Losers (Similar logic) ---
    url_l = movers_config['scrape_losers_url']
    logging.info(f"Attempting scrape: {url_l}")
    response_l = make_request(url_l)
    if response_l:
        try:
            soup = BeautifulSoup(response_l.text, 'html.parser')
            rows = soup.select(selectors['rows'])
            if not rows and selectors.get('fallback_rows'):
                logging.warning(f"Primary selector '{selectors['rows']}' failed for losers, trying fallback '{selectors['fallback_rows']}'")
                rows = soup.select(selectors['fallback_rows'])
            if not rows: logging.warning(f"Movers: Could not find table rows on {url_l} with configured selectors.")

            count = 0
            for i, row in enumerate(rows):
                if count >= limit: break
                try:
                    cells = row.find_all('td', recursive=False)
                    if len(cells) > max(selectors['symbol_cell_index'], selectors['name_cell_index'], selectors['change_percent_cell_index']):
                        symbol_tag = cells[selectors['symbol_cell_index']].select_one(selectors['symbol_tag_selector'])
                        name_tag_text = cells[selectors['name_cell_index']].get_text(strip=True)
                        change_percent_tag = cells[selectors['change_percent_cell_index']].select_one(selectors['change_percent_tag_selector'])

                        symbol = symbol_tag.text.strip() if symbol_tag else None
                        name = name_tag_text if name_tag_text else None
                        change_pct_val = None
                        if change_percent_tag:
                            change_pct_val_attr = change_percent_tag.get('value')
                            change_pct_val_text = change_percent_tag.get_text(strip=True).replace('%','').replace('+','')
                            try: change_pct_val = float(change_pct_val_attr) * 100
                            except (ValueError, TypeError):
                                try: change_pct_val = float(change_pct_val_text)
                                except (ValueError, TypeError): change_pct_val = None

                        if symbol and name and change_pct_val is not None:
                            movers['losers'].append({'symbol': symbol, 'name': name, 'change_percent': f"{change_pct_val:.2f}" })
                            count += 1
                        else: logging.debug(f"Losers: Missing data in row {i}. Sym:{symbol is not None}, Name:{name is not None}, Change:{change_pct_val is not None}")
                    else: logging.debug(f"Losers: Not enough cells ({len(cells)}) in row {i}")
                except Exception as e_row: logging.error(f"Error parsing losers row {i}: {e_row}", exc_info=False)

        except Exception as e: logging.error(f"Failed parsing losers page {url_l}: {e}", exc_info=True)
    else: logging.warning(f"Failed to fetch losers page: {url_l}")


    if not movers['gainers'] and not movers['losers']:
        logging.warning("Movers scraping failed completely.")
        return "Placeholder"
    logging.info(f"Movers data fetched (Gainers: {len(movers['gainers'])}, Losers: {len(movers['losers'])}).")
    return movers

def get_data_pool_for_topic(topic_key):
    """Gathers data using primary sources and secondary "hunt & seek" searches."""
    logging.info(f"--- Gathering data pool for topic: {topic_key} ---")
    topic_items = [] # List of dicts {title, url, source, text, resolved_url, type: 'primary'/'secondary'/'scraped'}
    processed_urls = set() # Track original URLs processed
    processed_resolved_urls = set() # Track resolved URLs scraped
    successful_scrapes = 0
    cfg_news = CONFIG['news_gathering']

    # === Layer 1: Primary Sources (NewsAPI if available, RSS) ===
    primary_items = []
    # NewsAPI (Optional)
    newsapi_queries = { # Reuse queries from v4.1
        'debt': '"corporate debt" OR "bond market" OR "interest rates" OR "credit spreads" OR yield',
        'risk': '"market risk" OR "economic outlook" OR recession OR inflation OR geopolitical',
        'deals': 'M&A OR acquisition OR merger OR buyout OR takeover',
        'trade': '"trade policy" OR tariff OR "commerce department" OR WTO OR "supply chain" OR import OR export',
        'commentary': '"market commentary" OR "stock analysis" OR "investor sentiment" OR "market outlook"',
        'general': 'business OR finance OR economy'
    }
    query = newsapi_queries.get(topic_key)
    if query and get_api_key('newsapi'): # Only run if key exists
        newsapi_results = fetch_newsapi_data(query, page_size=cfg_news['max_items_per_primary_source']) # Assuming fetch_newsapi_data defined as in v4.1
        if newsapi_results:
             logging.info(f"Adding {len(newsapi_results)} items from NewsAPI for {topic_key}")
             for item in newsapi_results:
                 if item.get('url') not in processed_urls:
                     item['type'] = 'primary'
                     item['resolved_url'] = resolve_redirect(item['url']) # Resolve early
                     primary_items.append(item)
                     processed_urls.add(item['url'])

    # RSS Feeds
    rss_feeds = cfg_news['rss_feeds'].get(topic_key, [])
    for feed_url in rss_feeds:
        source_name = f"RSS-{urlparse(feed_url).netloc}"
        rss_results = parse_rss_feed(feed_url, source_name=source_name, limit=cfg_news['max_items_per_primary_source'])
        if rss_results:
            logging.info(f"Adding {len(rss_results)} items from RSS {source_name} for {topic_key}")
            for item in rss_results:
                 if item.get('url') not in processed_urls:
                      item['type'] = 'primary'
                      # resolved_url should already be in item from parse_rss_feed
                      primary_items.append(item)
                      processed_urls.add(item['url'])
        time.sleep(CONFIG['polite_delay'])

    topic_items.extend(primary_items) # Add primary items to the main list

    # === Layer 2: Secondary "Hunt & Seek" Searches ===
    secondary_items = []
    if primary_items:
        # Extract keywords from primary results
        all_primary_text = " ".join([item['text'] for item in primary_items if item.get('text')])
        keywords = extract_keywords(all_primary_text, top_n=cfg_news['max_secondary_search_terms'])
        logging.info(f"Top keywords for secondary search ({topic_key}): {keywords}")

        for term in keywords:
            logging.info(f"Performing secondary Google News search for term: '{term}'")
            secondary_search_url = f"https://news.google.com/rss/search?q={requests.utils.quote(term)}+when:1d&hl=en-US&gl=US&ceid=US:en"
            secondary_results = parse_rss_feed(secondary_search_url, source_name=f"SecondarySearch-{term}", limit=cfg_news['max_items_per_secondary_search'])
            if secondary_results:
                 logging.info(f"Adding {len(secondary_results)} items from secondary search for '{term}'")
                 for item in secondary_results:
                      if item.get('url') not in processed_urls:
                           item['type'] = 'secondary'
                           secondary_items.append(item)
                           processed_urls.add(item['url'])
            time.sleep(CONFIG['polite_delay'])

    topic_items.extend(secondary_items) # Add secondary items

    # === Layer 3: Article Scraping (newspaper3k) ===
    # Try scraping URLs found in both primary and secondary items
    urls_to_attempt_scrape = list(dict.fromkeys([item.get('resolved_url') for item in topic_items if item.get('resolved_url')])) # Unique resolved URLs
    logging.info(f"Attempting newspaper3k scraping on up to {len(urls_to_attempt_scrape)} resolved URLs for {topic_key}...")

    scraped_data_map = {} # Store scraped data keyed by resolved URL

    for url in urls_to_attempt_scrape:
         if successful_scrapes >= cfg_news['max_articles_to_scrape_per_topic']: break
         if url and url not in processed_resolved_urls:
              scraped_content = scrape_article_text_newspaper(url, original_url="N/A") # Pass resolved URL
              if scraped_content:
                   scraped_data_map[url] = scraped_content # Store structured scrape result
                   successful_scrapes += 1
                   processed_resolved_urls.add(url)
              time.sleep(CONFIG['polite_delay']) # Be polite

    # Combine scraped text back into the main items list
    final_topic_items = []
    for item in topic_items:
         resolved_url = item.get('resolved_url')
         scraped_result = scraped_data_map.get(resolved_url) if resolved_url else None
         if scraped_result:
              # Use scraped data preferentially
              item['source'] = scraped_result['source'] # Update source to actual site
              item['title'] = scraped_result['title'] # Update title
              item['text'] = scraped_result['scraped_text'] # Replace summary with full text
              item['type'] = 'scraped'
         # Keep item even if scraping failed, use original text/title
         final_topic_items.append(item)


    logging.info(f"Collected {len(final_topic_items)} items for topic: {topic_key} ({successful_scrapes} successful scrapes)")
    # **FIX:** Always return a list, even if empty
    return final_topic_items if final_topic_items else []


# --- 8. Formatting Function (Ensure NEWSLETTER_TEMPLATE_V4 is defined) ---
NEWSLETTER_TEMPLATE_V4 = """
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>{{ newsletter_title }} - {{ date_short }}</title>
<style>
    body { margin: 0; padding: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; background-color: #f4f7f6; color: #333; }
    .container { max-width: 700px; margin: 20px auto; background-color: #ffffff; padding: 25px 35px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.05); }
    h1 { color: #1a0dab; font-size: 26px; margin-bottom: 10px; border-bottom: 2px solid #1a0dab; padding-bottom: 10px; }
    h2 { color: #333; font-size: 20px; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #e0e0e0; padding-bottom: 8px; }
    p, ul { line-height: 1.6; margin-bottom: 15px; font-size: 15px; }
    ul { list-style-type: none; padding-left: 0; }
    li { margin-bottom: 12px; padding-left: 1.5em; position: relative; }
    li::before { content: 'â–ª'; color: #0056b3; font-weight: bold; display: inline-block; width: 1em; margin-left: -1.5em; position: absolute; font-size: 1.1em; top: -1px; }
    a { color: #1a0dab; text-decoration: none; }
    a:hover { text-decoration: underline; }
    .section { margin-bottom: 25px; }
    .executive-summary { background-color: #e8f0fe; border-left: 4px solid #1a0dab; padding: 15px; margin: 25px 0; font-size: 1.05em; }
    .market-mood { text-align: center; font-size: 1.1em; font-weight: bold; padding: 10px; border-radius: 5px; margin: 20px 0; }
    .mood-positive { background-color: #e6f4ea; color: #155724; }
    .mood-negative { background-color: #f8d7da; color: #721c24; }
    .mood-mixed { background-color: #fff3cd; color: #856404; }
    .mood-neutral { background-color: #e2e3e5; color: #383d41; }
    .mood-error { background-color: #f8d7da; color: #721c24; }
    .data-spotlight { background-color: #f0f0f0; padding: 15px; border-radius: 5px; text-align: center; margin: 20px 0; }
    .data-spotlight .name { font-weight: bold; display: block; margin-bottom: 5px; font-size: 1.1em; }
    .data-spotlight .value { font-size: 1.4em; margin: 5px 0; display: block; }
    .data-spotlight .change { font-size: 1em; }
    .index-data li { margin-bottom: 10px; font-size: 0.95em; }
    .index-data .data-point { font-weight: 500; }
    .index-data .change-value { margin-left: 8px; }
    .positive { color: #28a745; }
    .negative { color: #dc3545; }
    .movers-section { display: flex; justify-content: space-between; flex-wrap: wrap;}
    .movers-section div { width: 48%; margin-bottom: 15px;}
    .movers-list li { font-size: 0.9em; margin-bottom: 6px; }
    .why-matters { font-style: italic; color: #555; border-left: 3px solid #ccc; padding-left: 10px; margin-top: 10px; font-size: 0.95em; }
    .what-to-watch li { font-size: 0.95em; }
    .placeholder { color: #6c757d; font-style: italic; }
    .error { color: #dc3545; font-weight: bold; }
    .footer { text-align: center; font-size: 0.8em; color: #777; margin-top: 30px; border-top: 1px solid #eee; padding-top: 15px; }
    .topic-section-content { max-height: 200px; overflow-y: auto; border: 1px solid #eee; padding: 10px; margin-top: 10px; font-size: 0.9em;} /* Added scrollable box for raw data */
    .topic-section-content li { font-size: 0.9em; margin-bottom: 5px; padding-left: 0; }
    .topic-section-content li::before { content: ''; } /* Remove default bullet */
</style>
</head>
<body>
<div class="container">
    <h1>{{ newsletter_title }}</h1>
    <p><strong>Date:</strong> {{ date_long }}</p>

    {% if executive_summary and 'Placeholder' not in executive_summary and 'Data gathering failed' not in executive_summary %}
    <div class="section executive-summary">
        <strong>Executive Summary:</strong> {{ executive_summary }}
    </div>
    {% elif executive_summary %}
     <div class="section executive-summary placeholder">
        <strong>Executive Summary:</strong> {{ executive_summary }}
    </div>
    {% endif %}

    {% if market_mood and market_mood != 'N/A' and market_mood != 'Error Analyzing' %}
    <div class="section market-mood mood-{{ market_mood.split('/')[0].split(' ')[-1].lower().replace('strongly','') }}"> Market Mood: {{ market_mood }}
    </div>
    {% elif market_mood == 'Error Analyzing' %}
    <div class="section market-mood mood-error">
         Market Mood: Error Analyzing Sentiment
    </div>
    {% endif %}

    {% if data_spotlight and data_spotlight.value != 'N/A' %}
    <div class="section data_spotlight">
        <span class="name">Data Spotlight: {{ data_spotlight.name }} ({{ data_spotlight.symbol }})</span>
        <span class="value">{{ data_spotlight.value }}</span>
        <span class="change {% if '+' in data_spotlight.change %}positive{% elif '-' in data_spotlight.change %}negative{% endif %}">{{ data_spotlight.change }}</span>
    </div>
    {% endif %}

    <div class="section">
        <h2>Headline Indices</h2>
        {% if index_data %}
        <ul class="index-data">
            {% for symbol, details in index_data.items() %}
                {% if details and details != 'Placeholder' %}
                <li>
                    <span class="data-point">{{ details.get('shortName', symbol) }}:</span>
                    {% if details.get('regularMarketPrice') is not none %}
                        <span class="data-point">{{ "%.2f"|format(details.get('regularMarketPrice')) }}</span>
                        {% if details.get('regularMarketChange') is not none and details.get('regularMarketChangePercent') is not none %}
                            <span class="change-value {{ 'negative' if details.get('regularMarketChange', 0) < 0 else 'positive' }}">
                                {{ "%+.2f"|format(details.get('regularMarketChange', 0)) }}
                                ({{ "%.2f"|format(details.get('regularMarketChangePercent', 0) * 100) }}%)
                            </span>
                        {% else %}<span class="placeholder">(Change N/A)</span>{% endif %}
                    {% else %}<span class="error">Price N/A</span>{% endif %}
                    {% if details.get('source') %}<small style="color:#888; margin-left:10px;">(Source: {{ details.source }})</small>{% endif %}
                </li>
                {% else %}
                 <li><span class="placeholder">Data for {{ symbol }} could not be retrieved.</span></li>
                {% endif %}
            {% endfor %}
        </ul>
        {% else %}<p class="error">Index data could not be retrieved.</p>{% endif %}
    </div>

    <div class="section">
        <h2>Major Stock Movers</h2>
        {% if stock_movers and stock_movers != 'Placeholder' %}
            <div class="movers-section">
                <div>
                    <strong>Top Gainers:</strong>
                    <ul class="movers-list">
                        {% for stock in stock_movers.get('gainers', []) %}
                            <li>{{ stock.get('symbol', 'N/A') }} <small>({{ stock.get('name', 'N/A')|truncate(30) }})</small> <span class="positive">(+{{ stock.get('change_percent', 'N/A') }}%)</span></li>
                        {% else %}<li><span class="placeholder">Gainers data unavailable.</span></li>{% endfor %}
                    </ul>
                </div>
                 <div>
                    <strong>Top Losers:</strong>
                     <ul class="movers-list">
                        {% for stock in stock_movers.get('losers', []) %}
                             <li>{{ stock.get('symbol', 'N/A') }} <small>({{ stock.get('name', 'N/A')|truncate(30) }})</small> <span class="negative">({{ stock.get('change_percent', 'N/A') }}%)</span></li>
                         {% else %}<li><span class="placeholder">Losers data unavailable.</span></li>{% endfor %}
                    </ul>
                </div>
            </div>
        {% else %}<p class="placeholder">Stock mover data could not be retrieved.</p>{% endif %}
         <p><small><em>Sector Notes: {{ sector_notes | default('N/A') }}</em></small></p>
    </div>

    {% macro render_topic_section(title, section_key, ai_content, why_matters_content, raw_data_items) %}
    <div class="section">
        <h2>{{ title }}</h2>
        {% if ai_content and 'Placeholder' not in ai_content and 'Data gathering failed' not in ai_content %}
            <p>{{ ai_content }}</p>
            {% if why_matters_content %}
                <p class="why-matters">{{ why_matters_content }}</p>
            {% endif %}
        {% elif ai_content %}
             <p class="placeholder">{{ ai_content }}</p> {# Display placeholder/error message #}
        {% else %}
             <p class="placeholder">Information for this section could not be automatically synthesized.</p>
        {% endif %}

        {# Optionally display raw data used for synthesis #}
        {% if raw_data_items %}
            <details>
                <summary style="cursor: pointer; font-size: 0.9em; color: #555;">Show Raw Data Snippets ({{ raw_data_items|length }})</summary>
                <ul class="topic-section-content">
                {% for item in raw_data_items %}
                    <li><strong>{{ item.get('source', 'N/A') }}:</strong> <a href="{{ item.get('resolved_url', item.get('url', '#')) }}" target="_blank">{{ item.get('title', 'N/A')|truncate(80) }}</a></li>
                {% endfor %}
                </ul>
            </details>
        {% endif %}
    </div>
    {% endmacro %}

    {# Render sections using the macro and passing the raw data pools #}
    {{ render_topic_section("Debt Market Update", "debt_news", debt_news, why_matters_debt, data_pools.get('debt', [])) }}
    {{ render_topic_section("Key Risks & Concerns", "negative_news", negative_news, why_matters_risk, data_pools.get('risk', [])) }}
    {{ render_topic_section("M&A and Corporate Activity", "ma_deals", ma_deals, None, data_pools.get('deals', [])) }}
    {{ render_topic_section("Tariffs & Trade Developments", "tariff_news", tariff_news, why_matters_trade, data_pools.get('trade', [])) }}
    {{ render_topic_section("Market Commentary & Outlook", "commentary", commentary, None, data_pools.get('commentary', [])) }}

    <div class="section">
        <h2>What to Watch Next Week</h2>
        {% if what_to_watch %}
        <ul class="what-to-watch">
            {% for item in what_to_watch %}
                <li>{{ item }}</li>
            {% endfor %}
        </ul>
        {% else %}<p class="placeholder">Outlook items could not be generated.</p>{% endif %}
    </div>

    <p class="footer">
        Generated by Market Mayhem Bot v4.2 on {{ date_long }} UTC. <br>
        Disclaimer: Automated summary for informational purposes only. Data sources include yfinance, RSS, web scraping; accuracy varies. AI synthesis is simulated. Verify critical information independently.
    </p>
</div>
</body>
</html>
"""

def format_newsletter(data):
    """Formats the fetched data into an HTML string using Jinja2."""
    logging.info("Formatting newsletter...")
    try:
        # Add a truncate filter manually
        def truncate_filter(s, length=250, killwords=False, end='...'):
             if s is None: return ''
             s = str(s)
             if len(s) <= length: return s
             if killwords: return s[:length-len(end)] + end
             words = s[:length-len(end)+1].rsplit(' ', 1)[0] # Split from right
             return words + end
        env = Environment()
        env.filters['truncate'] = truncate_filter
        template = env.from_string(NEWSLETTER_TEMPLATE_V4)
        return template.render(data)
    except Exception as e:
        logging.error(f"Failed to render Jinja2 template: {e}", exc_info=True)
        return f"<html><body><h1>Template Rendering Error</h1><p>{e}</p></body></html>"

# --- 9. Main Execution Block ---

if __name__ == "__main__":
    logging.info(f"====== Starting {CONFIG['output']['newsletter_title']} v4.2 Generation ======")
    start_time = time.time()
    run_datetime = datetime.now(timezone.utc)
    run_date_str = run_datetime.strftime('%Y-%m-%d')

    # --- Fetch Core Structured Data ---
    indices_data = get_indices_data()
    time.sleep(CONFIG['polite_delay'] / 2) # Shorten delay slightly
    movers_data = get_movers_data()
    time.sleep(CONFIG['polite_delay'] / 2)
    spotlight_data = get_data_spotlight()

    # --- Gather Raw Data Items (Including Hunt & Seek) ---
    topic_keys = ['debt', 'risk', 'deals', 'trade', 'commentary', 'general']
    data_pools = {} # Store the list of dicts for each topic
    for key in topic_keys:
        # **FIX:** Assign result to data_pools[key], call should return [] on failure
        data_pools[key] = get_data_pool_for_topic(key)
        time.sleep(CONFIG['polite_delay'])

    # --- Perform AI Processing (Simulated) ---
    ai_outputs = {}
    if CONFIG['ai_processing']['enabled']:
        logging.info("AI Processing Enabled (Using Simulated Outputs)")
        # Pass length of collected items list for context
        ai_outputs['debt_news'] = get_simulated_ai_output('debt_news', len(data_pools.get('debt', [])))
        ai_outputs['negative_news'] = get_simulated_ai_output('negative_news', len(data_pools.get('risk', [])))
        ai_outputs['ma_deals'] = get_simulated_ai_output('ma_deals', len(data_pools.get('deals', [])))
        ai_outputs['tariff_news'] = get_simulated_ai_output('tariff_news', len(data_pools.get('trade', [])))
        ai_outputs['commentary'] = get_simulated_ai_output('commentary', len(data_pools.get('commentary', [])))
        ai_outputs['executive_summary'] = get_simulated_ai_output('executive_summary') # Summary based on overall context
        ai_outputs['what_to_watch'] = get_simulated_ai_output('what_to_watch')
        ai_outputs['why_matters_trade'] = get_simulated_ai_output('why_it_matters_trade')
        ai_outputs['why_matters_risk'] = get_simulated_ai_output('why_it_matters_risk')
        ai_outputs['why_matters_debt'] = get_simulated_ai_output('why_it_matters_debt')

        # Sentiment analysis on combined pool items
        all_items = [item for pool in data_pools.values() for item in pool]
        ai_outputs['market_mood'] = get_sentiment_analysis(all_items)
        if ai_outputs['market_mood'] in ["N/A", "Error Analyzing"]:
             ai_outputs['market_mood'] = get_simulated_ai_output('market_mood')
    else:
        logging.warning("AI Processing Disabled.")
        # Populate with placeholders
        # ... (placeholder logic as before) ...


    # --- Prepare Final Data Object for Template ---
    final_template_data = {
        'date_long': run_datetime.strftime('%B %d, %Y %H:%M:%S %Z'),
        'date_short': run_date_str,
        'newsletter_title': CONFIG['output']['newsletter_title'],
        'index_data': indices_data,
        'stock_movers': movers_data,
        'data_spotlight': spotlight_data,
        'sector_notes': CONFIG['output']['sector_notes_placeholder'],
        'data_pools': data_pools, # Pass the raw data pools to the template
        **ai_outputs
    }

    # --- Format Newsletter ---
    newsletter_html = format_newsletter(final_template_data)

    # --- Output ---
    output_filename = f"Market_Mayhem_Newsletter_v4.2_{run_datetime.strftime('%Y%m%d_%H%M%S')}.html"
    # ... (Saving and Colab download logic as before) ...
    try:
        with open(output_filename, 'w', encoding='utf-8') as f: f.write(newsletter_html)
        logging.info(f"Newsletter saved to: {output_filename}")
        from google.colab import files
        print(f"\nAttempting Colab download for {output_filename}...")
        files.download(output_filename)
    except ImportError: logging.info("Not in Colab or files module failed.")
    except Exception as e: logging.error(f"Error saving/downloading file: {e}", exc_info=True)

    end_time = time.time()
    logging.info(f"====== Generation Complete ({end_time - start_time:.2f} seconds) ======")
