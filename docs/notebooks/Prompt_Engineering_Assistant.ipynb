{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Prompt Engineering Assistant\n",
    "**Version:** 2.0\n",
    "**Date:** August 18, 2025\n",
    "\n",
    "Welcome! This Jupyter Notebook is a multi-purpose tool designed to assist in the prompt engineering process for credit analysis. It serves as:\n",
    "1.  **A README:** Explaining its purpose, functionality, and limitations.\n",
    "2.  **A Prompt Engineering Guide:** Showing how a detailed prompt for a Large Language Model (LLM) is constructed.\n",
    "3.  **An Interactive Report Generation Tool:** Allowing you to input company data and generate a *simulated* credit report.\n",
    "4.  **An LLM-powered report generator:** Allowing you to generate a report using an LLM.\n",
    "5.  **A Feedback and Evaluation Tool:** Allowing you to score and provide feedback on the LLM-generated report.\n",
    "6.  **A Data Output Guide:** Showing an example JSONL format for storing report data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. README: Understanding This Notebook\n",
    "\n",
    "### 1.1. Purpose\n",
    "This notebook aims to streamline the initial phases of corporate credit report generation by:\n",
    "* Providing a structured way to input key quantitative and qualitative data.\n",
    "* Automating the construction of a comprehensive prompt for a sophisticated Large Language Model (LLM).\n",
    "* Generating a 'first-pass' simulated credit report based on the inputs and a rule-based internal logic, which can serve as a starting point for further analysis or be used to understand the LLM prompt's intent.\n",
    "\n",
    "### 1.2. How it Works\n",
    "The process is straightforward:\n",
    "1.  **User Input:** You provide company-specific data through interactive widgets (see Section 3).\n",
    "2.  **Prompt Construction:** The notebook takes your inputs and dynamically builds a detailed, structured prompt designed to guide an advanced LLM.\n",
    "3.  **Simulated Report Generation:** An internal Python class (`DynamicReportSimulator`) processes your inputs and the predefined report structure to generate a Markdown-formatted credit report. **Crucially, this is a simulation and does not use an external LLM.**\n",
    "4.  **Output Review:** You can review both the generated LLM prompt and the simulated report.\n",
    "\n",
    "### 1.3. Key Components\n",
    "* **Input UI:** A series of `ipywidgets` for easy data entry.\n",
    "* **Prompt Engine:** Python logic that merges user inputs with a core prompt template and a report structure guide.\n",
    "* **Report Simulator (`DynamicReportSimulator`):** Python class that attempts to generate a structured report based on the inputs. Its logic is rule-based and illustrative.\n",
    "\n",
    "### 1.4. How to Use\n",
    "1.  Run the code cells in Section 3 to display the interactive input fields.\n",
    "2.  Carefully fill in the details for the company you wish to analyze.\n",
    "3.  Click the \"Generate Full Prompt & Simulated Report\" button.\n",
    "4.  The notebook will output the detailed LLM prompt and the simulated Markdown report below the button.\n",
    "5.  Refer to the examples in Section 4 (Microsoft Report) and Section 5 (Report Review) to understand the output quality and potential applications.\n",
    "\n",
    "### 1.5. Limitations\n",
    "* **No External LLM Calls:** This notebook, for portability and simplicity, *simulates* report generation. It does **not** make calls to external LLM APIs (like OpenAI, Google Gemini, etc.). The simulated report is based on internal logic and the structure you provide.\n",
    "* **Simulation Quality:** The `DynamicReportSimulator` uses a rule-based approach. Its output is intended to be a structured first draft and will not have the nuanced reasoning, creativity, or broad knowledge of a state-of-the-art LLM. Its quality is directly tied to its programmed logic and the quality of your inputs.\n",
    "* **Input Dependency:** The comprehensiveness of the output heavily depends on the detail and accuracy of the information you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering Guide\n",
    "\n",
    "Understanding how the LLM prompt is constructed can help you provide better inputs and interpret the notebook's functionality. The final prompt sent to a (conceptual) LLM is built from three main parts: a core template, your specific inputs, and a report structure guide.\n",
    "\n",
    "### 2.1. The Core LLM Prompt Template\n",
    "This template provides the overarching instructions, role, and task for the LLM. Here's the content (it's also embedded in the Python code):\n",
    "```text\n",
    "You are an expert senior credit analyst AI, tasked with generating a comprehensive, balanced, and insightful corporate credit report. Your analysis should be objective and data-driven, drawing upon the information provided below.\n",
    "\n",
    "**Objective:** Produce a corporate credit report for the specified company, adhering to the structure outlined in the \"REPORT_STRUCTURE_GUIDE\".\n",
    "\n",
    "**Key Instructions:**\n",
    "1.  **Company Focus:** The report is for: {company_name} ({company_ticker}), operating in the {company_sector} sector.\n",
    "2.  **Information Provided by User:** You will receive structured inputs including:\n",
    "    * Key Financial Data\n",
    "    * Calculated Credit Metrics\n",
    "    * Qualitative Assessments (Management, Competitive Landscape, Industry Outlook, ESG Factors)\n",
    "    * Recent News Snippets / Press Releases\n",
    "    * Analyst's Key Assumptions\n",
    "3.  **Analysis Approach:**\n",
    "    * Synthesize all provided quantitative and qualitative information.\n",
    "    * Identify key credit strengths and weaknesses.\n",
    "    * Discuss financial performance and creditworthiness based on the data.\n",
    "    * Incorporate recent developments and their potential impact.\n",
    "    * Clearly state the rating rationale and outlook based *only* on the information provided.\n",
    "    * If specific data for a standard report section is NOT provided, explicitly state \"Information not provided for this section.\" or \"Analysis for this section is limited due to lack of specific input.\" Do NOT invent data.\n",
    "4.  **Tone:** Professional, analytical, objective, and cautious. Use clear and concise language.\n",
    "5.  **Output Format:** Generate the report in Markdown format, strictly following the section headers and structure provided in the \"REPORT_STRUCTURE_GUIDE\".\n",
    "6.  **Disclaimer:** Conclude the report with the mandatory disclaimer: \"This report is a simulated analysis generated based on user-provided inputs and should not be used for actual investment or credit decisions. Verify all information independently.\"\n",
    "\n",
    "**USER-PROVIDED INFORMATION WILL BE INSERTED BELOW THIS LINE WHEN THE FULL PROMPT IS CONSTRUCTED.**\n",
    "---INPUT_DATA_MARKER---\n",
    "**REPORT_STRUCTURE_GUIDE:**\n",
    "{report_structure_guide}\n",
    "---END_REPORT_STRUCTURE_GUIDE---\n",
    "**FINAL INSTRUCTION: Now, generate the comprehensive corporate credit report based on all the above instructions and the provided user inputs, adhering strictly to the REPORT_STRUCTURE_GUIDE.**\n",
    "```\n",
    "*Explainer: Placeholders like `{company_name}` are filled dynamically. The `---INPUT_DATA_MARKER---` is where your structured inputs go. The `{report_structure_guide}` is replaced by the content below.* \n",
    "\n",
    "### 2.2. The Report Structure Guide\n",
    "This Markdown content is embedded within the main prompt to tell the LLM exactly how to format its output. This ensures consistency. (Content is also embedded in Python code):\n",
    "```markdown\n",
    "# Corporate Credit Report: {company_name} ({company_ticker})\n",
    "\n",
    "## 1. Executive Summary\n",
    "    * Overall Assessment: [Brief summary of creditworthiness based on inputs]\n",
    "    * Simulated Credit Rating: [e.g., BBB+, A-, etc. - LLM should infer a plausible rating based on inputs OR state if inputs are insufficient to determine]\n",
    "    * Rating Outlook: [e.g., Stable, Positive, Negative - LLM should infer a plausible outlook OR state if inputs are insufficient]\n",
    "    * Key Positive Factors: [List 2-3 key strengths from inputs]\n",
    "    * Key Credit Concerns: [List 2-3 key risks/weaknesses from inputs]\n",
    "\n",
    "## 2. Company Overview\n",
    "    * Company Name: {company_name}\n",
    "    * Ticker Symbol: {company_ticker}\n",
    "    * Primary Sector: {company_sector}\n",
    "    * Brief Business Description (if provided by user): {qualitative_business_description}\n",
    "...\n",
    "## 11. Disclaimer\n",
    "This report is a simulated analysis generated based on user-provided inputs and should not be used for actual investment or credit decisions. Verify all information independently.\n",
    "```\n",
    "*Explainer: The full structure guide (as shown in earlier examples) is used here. `{placeholders}` within this guide are intended for the LLM to fill OR for the `DynamicReportSimulator` to populate.*\n",
    "\n",
    "### 2.3. How Your Inputs Shape the Prompt\n",
    "The information you enter into the UI widgets is formatted and inserted into the main prompt under the `---INPUT_DATA_MARKER---` section. For example:\n",
    "* **Company Information:** Directly populates `{company_name}`, `{company_ticker}`, etc.\n",
    "* **Financial Data & Metrics:** Listed clearly for the LLM to reference.\n",
    "* **Qualitative Sections:** Your text for 'Management Assessment', 'Competitive Landscape', etc., is provided verbatim.\n",
    "\n",
    "### 2.4. Tips for Effective Inputs\n",
    "* **Be Specific:** For financials and metrics, use accurate numbers. For qualitative points, avoid vague statements.\n",
    "* **Quantitative Data:** The more precise your financial inputs, the better the (conceptual) LLM can ground its analysis. The internal simulator also uses these for its simple rules.\n",
    "* **Qualitative Nuance:** For sections like 'Management Assessment' or 'Competitive Landscape', provide balanced views if possible (e.g., strengths and weaknesses).\n",
    "* **Relevant News:** Include recent, impactful news. Summarize if necessary.\n",
    "* **Clear Assumptions:** Well-defined assumptions are crucial for any analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Report Generator\n",
    "Run the code cell below to display the interactive input fields. Fill them out for the company you want to analyze, then click the \"Generate\" button. The LLM prompt and the simulated report will appear underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, FileLink\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import base64\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    print(\"OpenAI library not installed. Please install with 'pip install openai'\")\n",
    "    OpenAI = None\n",
    "\n",
    "try:\n",
    "    import pypdfium2 as pdfium\n",
    "except ImportError:\n",
    "    print(\"pypdfium2 library not installed. Please install with 'pip install pypdfium2'\")\n",
    "    pdfium = None\n",
    "\n",
    "PROMPT_TEMPLATE_CORE = \"\"\"You are an expert senior credit analyst AI, tasked with generating a comprehensive, balanced, and insightful corporate credit report. Your analysis should be objective and data-driven, drawing upon the information provided below.\n",
    "\n",
    "**Objective:** Produce a corporate credit report for the specified company, adhering to the structure outlined in the \\\"REPORT_STRUCTURE_GUIDE\\\".\n",
    "\n",
    "**Key Instructions:**\n",
    "1.  **Company Focus:** The report is for: {company_name} ({company_ticker}), operating in the {company_sector} sector.\n",
    "2.  **Information Provided by User:** You will receive structured inputs including:\n",
    "    * Key Financial Data\n",
    "    * Calculated Credit Metrics\n",
    "    * Qualitative Assessments (Management, Competitive Landscape, Industry Outlook, ESG Factors)\n",
    "    * Recent News Snippets / Press Releases\n",
    "    * Analyst's Key Assumptions\n",
    "    * Additional context from uploaded documents (if provided by user): {uploaded_document_context}\n",
    "3.  **Analysis Approach:**\n",
    "    * Synthesize all provided quantitative and qualitative information, including any text from uploaded documents.\n",
    "    * Identify key credit strengths and weaknesses.\n",
    "    * Discuss financial performance and creditworthiness based on the data.\n",
    "    * Incorporate recent developments and their potential impact.\n",
    "    * Clearly state the rating rationale and outlook based *only* on the information provided.\n",
    "    * If specific data for a standard report section is NOT provided, explicitly state \\\"Information not provided for this section.\\\" or \\\"Analysis for this section is limited due to lack of specific input.\\\" Do NOT invent data.\n",
    "4.  **Tone:** Professional, analytical, objective, and cautious. Use clear and concise language.\n",
    "5.  **Output Format:** Generate the report in Markdown format, strictly following the section headers and structure provided in the \\\"REPORT_STRUCTURE_GUIDE\\\".\n",
    "6.  **Disclaimer:** Conclude the report with the mandatory disclaimer: \\\"This report is an AI-generated analysis based on user-provided inputs and should not be used for actual investment or credit decisions without independent verification by a qualified human professional. Verify all information independently.\\\"\n",
    "\n",
    "**USER-PROVIDED INFORMATION WILL BE INSERTED BELOW THIS LINE WHEN THE FULL PROMPT IS CONSTRUCTED.**\n",
    "---INPUT_DATA_MARKER---\n",
    "**REPORT_STRUCTURE_GUIDE:**\n",
    "{report_structure_guide}\n",
    "---END_REPORT_STRUCTURE_GUIDE---\n",
    "**FINAL INSTRUCTION: Now, generate the comprehensive corporate credit report based on all the above instructions and the provided user inputs, adhering strictly to the REPORT_STRUCTURE_GUIDE.**\n",
    "\"\"\"\n",
    "\n",
    "REPORT_STRUCTURE_GUIDE = \"\"\"# Corporate Credit Report: {company_name} ({company_ticker})\n",
    "\n",
    "## 1. Executive Summary\n",
    "    * Overall Assessment: [Brief summary of creditworthiness based on inputs]\n",
    "    * Suggested Credit Rating: [e.g., BBB+, A-, etc. - LLM should infer a plausible rating based on inputs OR state if inputs are insufficient to determine]\n",
    "    * Rating Outlook: [e.g., Stable, Positive, Negative - LLM should infer a plausible outlook OR state if inputs are insufficient]\n",
    "    * Key Positive Factors: [List 2-3 key strengths from inputs]\n",
    "    * Key Credit Concerns: [List 2-3 key risks/weaknesses from inputs]\n",
    "\n",
    "## 2. Company Overview\n",
    "    * Company Name: {company_name}\n",
    "    * Ticker Symbol: {company_ticker}\n",
    "    * Primary Sector: {company_sector}\n",
    "    * Brief Business Description (if provided by user): {qualitative_business_description}\n",
    "    * Additional Context from Uploaded Documents: [Summarize relevant points from uploaded_document_context IF PROVIDED, otherwise state \"No documents uploaded for additional context.\"]\n",
    "\n",
    "## 3. Key Analyst Assumptions\n",
    "    * [List of key assumptions provided by the user: {key_assumptions}]\n",
    "\n",
    "## 4. Financial Performance Analysis\n",
    "    * Summary of Provided Financials:\n",
    "        * {financial_data_summary}\n",
    "    * Key Credit Metrics Analysis:\n",
    "        * {credit_metrics_summary}\n",
    "    * Trend Analysis (based on provided data interpretation):\n",
    "        * [Comment on trends if discernible from inputs]\n",
    "\n",
    "## 5. Qualitative Factors Assessment\n",
    "    * Management & Strategy:\n",
    "        * {qualitative_management_strategy}\n",
    "    * Competitive Landscape & Market Position:\n",
    "        * {qualitative_competitive_landscape}\n",
    "    * Industry Outlook:\n",
    "        * {qualitative_industry_outlook}\n",
    "    * Environmental, Social, and Governance (ESG) Considerations (if provided):\n",
    "        * {qualitative_esg_factors}\n",
    "\n",
    "## 6. Recent Developments & News\n",
    "    * Summary of Recent Press Releases/News:\n",
    "        * {recent_news_summary}\n",
    "    * Potential Impact Analysis (based on analyst input or LLM inference if obvious from news):\n",
    "        * [Briefly discuss potential credit implications of the news]\n",
    "\n",
    "## 7. Credit Strengths\n",
    "    * [Synthesized from all inputs - financial, qualitative, news, uploaded documents etc.]\n",
    "\n",
    "## 8. Credit Risks & Concerns\n",
    "    * [Synthesized from all inputs - financial, qualitative, news, uploaded documents etc.]\n",
    "\n",
    "## 9. Rating Rationale\n",
    "    * [Explain the reasoning behind the suggested rating, tying back to specific strengths, weaknesses, financial metrics, and qualitative factors provided.]\n",
    "\n",
    "## 10. Outlook Rationale\n",
    "    * [Explain the reasoning behind the suggested outlook, considering potential trends and future impacts of current factors.]\n",
    "\n",
    "## 11. Disclaimer\n",
    "This report is an AI-generated analysis based on user-provided inputs and should not be used for actual investment or credit decisions without independent verification by a qualified human professional. Verify all information independently.\n",
    "\"\"\"\n",
    "INPUT_UI_CONFIG = {\n",
    "  \"sections\": [\n",
    "    {\"title\": \"Company Information\", \"fields\": [\n",
    "        {\"name\": \"company_name\", \"label\": \"Company Name:\", \"type\": \"text\", \"default\": \"ExampleCorp\"},\n",
    "        {\"name\": \"company_ticker\", \"label\": \"Ticker Symbol:\", \"type\": \"text\", \"default\": \"EXMPL\"},\n",
    "        {\"name\": \"company_sector\", \"label\": \"Primary Sector:\", \"type\": \"text\", \"default\": \"Technology\"}]},\n",
    "    {\"title\": \"Key Analyst Assumptions\", \"fields\": [\n",
    "        {\"name\": \"key_assumptions\", \"label\": \"Analyst's Key Assumptions:\", \"type\": \"textarea\", \"default\": \"1. Moderate revenue growth.\\n2. Stable margins.\"}]},\n",
    "    {\"title\": \"Financial Data (Illustrative)\", \"fields\": [\n",
    "        {\"name\": \"financial_revenue_y1\", \"label\": \"Recent Full Year Revenue ($M):\", \"type\": \"float\", \"default\": 1000.0},\n",
    "        {\"name\": \"financial_ebitda_y1\", \"label\": \"Recent Full Year EBITDA ($M):\", \"type\": \"float\", \"default\": 250.0},\n",
    "        {\"name\": \"financial_total_debt_y1\", \"label\": \"Total Debt ($M):\", \"type\": \"float\", \"default\": 500.0},\n",
    "        {\"name\": \"financial_total_equity_y1\", \"label\": \"Total Equity ($M):\", \"type\": \"float\", \"default\": 300.0},\n",
    "        {\"name\": \"financial_fcf_y1\", \"label\": \"Free Cash Flow ($M):\", \"type\": \"float\", \"default\": 50.0}]},\n",
    "    {\"title\": \"Credit Metrics (Illustrative)\", \"fields\": [\n",
    "        {\"name\": \"metric_debt_ebitda\", \"label\": \"Debt / EBITDA (x):\", \"type\": \"float\", \"default\": 2.0},\n",
    "        {\"name\": \"metric_ebitda_interest\", \"label\": \"EBITDA / Interest Expense (x):\", \"type\": \"float\", \"default\": 5.0}]},\n",
    "    {\"title\": \"Qualitative Factors\", \"fields\": [\n",
    "        {\"name\": \"qualitative_business_description\", \"label\": \"Brief Business Description:\", \"type\": \"textarea\", \"default\": \"Leading provider of widgets.\"},\n",
    "        {\"name\": \"qualitative_management_strategy\", \"label\": \"Management Assessment & Strategy:\", \"type\": \"textarea\", \"default\": \"Experienced management.\"}, \n",
    "        {\"name\": \"qualitative_competitive_landscape\", \"label\": \"Competitive Landscape & Market Position:\", \"type\": \"textarea\", \"default\": \"Competitive market.\"},\n",
    "        {\"name\": \"qualitative_industry_outlook\", \"label\": \"Industry Outlook:\", \"type\": \"textarea\", \"default\": \"Modest growth expected.\"},\n",
    "        {\"name\": \"qualitative_esg_factors\", \"label\": \"ESG Considerations:\", \"type\": \"textarea\", \"default\": \"Standard ESG practices.\"}]},\n",
    "    {\"title\": \"Recent News / Press Releases\", \"fields\": [\n",
    "        {\"name\": \"recent_news_summary\", \"label\": \"Paste recent news snippets/summaries here:\", \"type\": \"textarea\", \"default\": \"Q1 earnings meet expectations.\"}]}]}\n",
    "\n",
    "\n",
    "class DynamicReportSimulator:\n",
    "    def __init__(self, report_structure_template_md):\n",
    "        self.report_structure_template_md = report_structure_template_md\n",
    "\n",
    "    def _format_financials(self, inputs_dict):\n",
    "        summary = []\n",
    "        for key, value in inputs_dict.items():\n",
    "            if key.startswith('financial_') and value is not None:\n",
    "                label = key.replace('financial_', '').replace('_', ' ').title()\n",
    "                if 'Revenue' in label or 'Ebitda' in label or 'Debt' in label or 'Equity' in label or 'Fcf' in label:\n",
    "                    summary.append(f\"- {label}: ${value}M\")\n",
    "                else:\n",
    "                    summary.append(f\"- {label}: {value}\")\n",
    "        return \"\\n        \".join(summary) if summary else \"No specific financial figures provided.\"\n",
    "\n",
    "    def _format_metrics(self, inputs_dict):\n",
    "        summary = []\n",
    "        commentary = []\n",
    "        for key, value in inputs_dict.items():\n",
    "            if key.startswith('metric_') and value is not None:\n",
    "                label = key.replace('metric_', '').replace('_', ' ').upper()\n",
    "                summary.append(f\"- {label}: {value}x\")\n",
    "                if 'Debt / Ebitda' in label:\n",
    "                    try:\n",
    "                        val = float(value)\n",
    "                        if val <= 2.0: commentary.append(\"Leverage (Debt/EBITDA) appears low to moderate.\")\n",
    "                        elif val <= 4.0: commentary.append(\"Leverage (Debt/EBITDA) appears moderate.\")\n",
    "                        else: commentary.append(\"Leverage (Debt/EBITDA) appears high.\")\n",
    "                    except ValueError: commentary.append(\"Invalid Debt/EBITDA value.\")\n",
    "                if 'Ebitda / Interest' in label:\n",
    "                    try:\n",
    "                        val = float(value)\n",
    "                        if val >= 5.0: commentary.append(\"Interest coverage appears strong.\")\n",
    "                        elif val >= 2.0: commentary.append(\"Interest coverage appears adequate.\")\n",
    "                        else: commentary.append(\"Interest coverage appears weak.\")\n",
    "                    except ValueError: commentary.append(\"Invalid EBITDA/Interest value.\")\n",
    "\n",
    "        metrics_text = \"\\n        \".join(summary) if summary else \"No specific credit metrics provided.\"\n",
    "        if commentary:\n",
    "            metrics_text += \"\\n    * **Brief Commentary:**\\n        * \" + \"\\n        * \".join(commentary)\n",
    "        return metrics_text\n",
    "\n",
    "    def _infer_rating_outlook(self, inputs_dict):\n",
    "        rating = \"BBB\"; outlook = \"Stable\"; score = 0\n",
    "        try:\n",
    "            if inputs_dict.get('financial_fcf_y1', 0.0) > 0: score += 1\n",
    "            else: score -=1\n",
    "            if inputs_dict.get('financial_revenue_y1', 0.0) > 500 : score +=1\n",
    "            debt_ebitda = inputs_dict.get('metric_debt_ebitda')\n",
    "            if debt_ebitda is not None:\n",
    "                val = float(debt_ebitda)\n",
    "                if val < 1.5: score += 2\n",
    "                elif val < 3.0: score += 1\n",
    "                elif val > 4.5: score -= 2\n",
    "                else: score -=1\n",
    "            ebitda_interest = inputs_dict.get('metric_ebitda_interest')\n",
    "            if ebitda_interest is not None:\n",
    "                val = float(ebitda_interest)\n",
    "                if val > 8.0: score += 2\n",
    "                elif val > 4.0: score += 1\n",
    "                elif val < 2.0: score -=2\n",
    "                else: score -=1\n",
    "            qual_texts = [inputs_dict.get('qualitative_management_strategy',''), inputs_dict.get('qualitative_competitive_landscape',''), inputs_dict.get('qualitative_industry_outlook','')]\n",
    "            negative_keywords = [\"poor\", \"declining\", \"weak\", \"intense competition\", \"headwinds\", \"challenging\"]\n",
    "            for text in qual_texts:\n",
    "                for keyword in negative_keywords:\n",
    "                    if keyword in text.lower(): score -=1; break\n",
    "            if \"strong growth\" in inputs_dict.get('qualitative_industry_outlook','').lower() : score +=1\n",
    "            if \"strong execution\" in inputs_dict.get('qualitative_management_strategy','').lower() : score +=1\n",
    "        except Exception: pass # Simplified error handling\n",
    "        if score >= 4: rating = \"A-\"; outlook = \"Positive\"\n",
    "        elif score >= 2: rating = \"BBB+\"; outlook = \"Stable\"\n",
    "        elif score >= 0: rating = \"BBB\"; outlook = \"Stable\"\n",
    "        elif score >= -2: rating = \"BBB-\"; outlook = \"Negative\"\n",
    "        elif score >= -4: rating = \"BB+\"; outlook = \"Negative\"\n",
    "        else: rating = \"BB\"; outlook = \"Negative\"\n",
    "        return rating, outlook\n",
    "\n",
    "    def generate_simulated_report(self, inputs_dict):\n",
    "        sim_rating, sim_outlook = self._infer_rating_outlook(inputs_dict)\n",
    "        positive_factors = []; credit_concerns = []\n",
    "        try:\n",
    "            if inputs_dict.get('financial_fcf_y1', 0.0) > 0: positive_factors.append(f\"Positive Free Cash Flow (${inputs_dict.get('financial_fcf_y1', 'N/A')}M).\")\n",
    "            else: credit_concerns.append(f\"Negative or low Free Cash Flow (${inputs_dict.get('financial_fcf_y1', 'N/A')}M).\")\n",
    "            debt_ebitda = inputs_dict.get('metric_debt_ebitda')\n",
    "            if debt_ebitda is not None:\n",
    "                val = float(debt_ebitda)\n",
    "                if val < 2.0: positive_factors.append(f\"Low leverage (Debt/EBITDA: {debt_ebitda}x).\")\n",
    "                elif val > 4.0: credit_concerns.append(f\"High leverage (Debt/EBITDA: {debt_ebitda}x).\")\n",
    "            if \"strong execution\" in inputs_dict.get('qualitative_management_strategy','').lower(): positive_factors.append(\"Indication of strong management execution.\")\n",
    "            if \"intense competition\" in inputs_dict.get('qualitative_competitive_landscape','').lower(): credit_concerns.append(\"Intense competitive landscape noted.\")\n",
    "        except Exception:\n",
    "            positive_factors.append(\"Error processing positive factors.\")\n",
    "            credit_concerns.append(\"Error processing credit concerns.\")\n",
    "\n",
    "        if not positive_factors: positive_factors.append(\"No specific positive factors highlighted from inputs.\")\n",
    "        if not credit_concerns: credit_concerns.append(\"No specific credit concerns highlighted from inputs.\")\n",
    "\n",
    "        report_replacements = {k: inputs_dict.get(k, 'N/A') for k in ['company_name', 'company_ticker', 'company_sector', 'qualitative_business_description', 'qualitative_management_strategy', 'qualitative_competitive_landscape', 'qualitative_industry_outlook', 'qualitative_esg_factors']}\n",
    "        report_replacements['key_assumptions'] = inputs_dict.get('key_assumptions', 'N/A').replace('\\n', '\\n    * ')\n",
    "        report_replacements['financial_data_summary'] = self._format_financials(inputs_dict)\n",
    "        report_replacements['credit_metrics_summary'] = self._format_metrics(inputs_dict)\n",
    "        report_replacements['recent_news_summary'] = inputs_dict.get('recent_news_summary', 'N/A').replace('\\n', '\\n        * ')\n",
    "        report_replacements['uploaded_document_context'] = inputs_dict.get('uploaded_document_text', \"No documents uploaded for additional context.\")\n",
    "\n",
    "\n",
    "        report_md = self.report_structure_template_md\n",
    "        for key, value in report_replacements.items():\n",
    "            report_md = report_md.replace(f\"{{{key}}}\", str(value))\n",
    "\n",
    "        overall_assessment_text = f\"Based on the provided inputs, the company exhibits characteristics consistent with a {sim_rating} credit profile. Financial metrics show leverage at {inputs_dict.get('metric_debt_ebitda', 'N/A')}x and interest coverage at {inputs_dict.get('metric_ebitda_interest', 'N/A')}x. Qualitative factors and recent news contribute to this view. The outlook is {sim_outlook}.\"\n",
    "        report_md = report_md.replace(\"Overall Assessment: [Brief summary of creditworthiness based on inputs]\", f\"Overall Assessment: {overall_assessment_text}\")\n",
    "        report_md = report_md.replace(\"Suggested Credit Rating: [e.g., BBB+, A-, etc. - LLM should infer a plausible rating based on inputs OR state if inputs are insufficient to determine]\", f\"Suggested Credit Rating: {sim_rating} (Simulated)\")\n",
    "        report_md = report_md.replace(\"Rating Outlook: [e.g., Stable, Positive, Negative - LLM should infer a plausible outlook OR state if inputs are insufficient]\", f\"Rating Outlook: {sim_outlook} (Simulated)\")\n",
    "        report_md = report_md.replace(\"Key Positive Factors: [List 2-3 key strengths from inputs]\", f\"Key Positive Factors:\\n        * \" + \"\\n        * \".join(positive_factors))\n",
    "        report_md = report_md.replace(\"Key Credit Concerns: [List 2-3 key risks/weaknesses from inputs]\", f\"Key Credit Concerns:\\n        * \" + \"\\n        * \".join(credit_concerns))\n",
    "\n",
    "        rating_rationale_text = f\"The simulated rating of {sim_rating} is primarily driven by the interplay of its financial metrics (e.g., Debt/EBITDA of {inputs_dict.get('metric_debt_ebitda', 'N/A')}x, FCF of ${inputs_dict.get('financial_fcf_y1', 'N/A')}M), and qualitative assessments. The balance of these factors, as provided, supports this assessment.\"\n",
    "        report_md = report_md.replace(\"[Explain the reasoning behind the suggested rating, tying back to specific strengths, weaknesses, financial metrics, and qualitative factors provided.]\", rating_rationale_text)\n",
    "        outlook_rationale_text = f\"The {sim_outlook} outlook is based on the current trajectory implied by the financial data, stated key assumptions, and potential impacts of recent news and industry trends. Stability (or change) in key credit metrics will be key determinants.\"\n",
    "        report_md = report_md.replace(\"[Explain the reasoning behind the suggested outlook, considering potential trends and future impacts of current factors.]\", outlook_rationale_text)\n",
    "        report_md = report_md.replace(\"[Comment on trends if discernible from inputs]\", \"[Detailed trend analysis requires time-series data. Based on single-period input, this section focuses on current state.]\")\n",
    "        report_md = report_md.replace(\"[Briefly discuss potential credit implications of the news]\", \"[The provided news snippets could impact credit quality. Further analysis needed.]\")\n",
    "        report_md = report_md.replace(\"[Synthesized from all inputs - financial, qualitative, news, uploaded documents etc.]\", \"[This section would typically synthesize all inputs. Key strengths identified include: \" + \", \".join(positive_factors) + \". Further synthesis pending.]\")\n",
    "        risk_synthesis = \"[Key risks identified include: \" + \", \".join(credit_concerns) + \". Further synthesis pending.]\"\n",
    "        report_md = report_md.replace(\"## 8. Credit Risks & Concerns\\n    * [Synthesized from all inputs - financial, qualitative, news, uploaded documents etc.]\", f\"## 8. Credit Risks & Concerns\\n    * {risk_synthesis}\")\n",
    "\n",
    "\n",
    "        return report_md\n",
    "\n",
    "report_simulator = DynamicReportSimulator(REPORT_STRUCTURE_GUIDE)\n",
    "\n",
    "current_llm_prompt = \"\"\n",
    "current_llm_report_md = \"\"\n",
    "current_simulated_report_md = \"\"\n",
    "current_user_inputs = {}\n",
    "current_uploaded_text = \"\"\n",
    "\n",
    "api_key_input = widgets.PasswordText(description='OpenAI API Key:', layout=widgets.Layout(width='500px'))\n",
    "llm_model_select = widgets.Dropdown(\n",
    "    options=['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo-preview'],\n",
    "    value='gpt-3.5-turbo',\n",
    "    description='LLM Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "file_upload_widget = widgets.FileUpload(\n",
    "    accept='.pdf,.txt',\n",
    "    multiple=False,\n",
    "    description='Upload PDF/TXT'\n",
    ")\n",
    "uploaded_file_name_display = widgets.Label(value=\"No file uploaded.\")\n",
    "\n",
    "input_widgets = {}\n",
    "sections_vbox_list = []\n",
    "for section_conf in INPUT_UI_CONFIG['sections']:\n",
    "    section_title_html = widgets.HTML(f\"<h3>{section_conf['title']}</h3>\")\n",
    "    fields_vbox_list_inner = [section_title_html]\n",
    "    for field_config in section_conf['fields']:\n",
    "        label_widget = widgets.Label(field_config['label'], layout=widgets.Layout(width='250px'))\n",
    "        if field_config['type'] == 'text': widget_item = widgets.Text(value=str(field_config['default']), layout=widgets.Layout(width='70%'))\n",
    "        elif field_config['type'] == 'textarea': widget_item = widgets.Textarea(value=str(field_config['default']), layout=widgets.Layout(width='70%', height='80px'))\n",
    "        elif field_config['type'] == 'float': widget_item = widgets.FloatText(value=float(field_config['default']), layout=widgets.Layout(width='auto'))\n",
    "        else: widget_item = widgets.Text(value=str(field_config['default']), layout=widgets.Layout(width='70%'))\n",
    "        input_widgets[field_config['name']] = widget_item\n",
    "        fields_vbox_list_inner.append(widgets.HBox([label_widget, widget_item]))\n",
    "    sections_vbox_list.append(widgets.VBox(fields_vbox_list_inner, layout=widgets.Layout(margin='0 0 20px 0')))\n",
    "\n",
    "all_inputs_vbox = widgets.VBox(sections_vbox_list)\n",
    "\n",
    "output_format_dropdown = widgets.Dropdown(\n",
    "    options=['Markdown', 'JSON', 'YAML'],\n",
    "    value='Markdown',\n",
    "    description='Output Format:',\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(description=\"Generate Prompt & Reports (Sim + LLM)\", button_style='success', layout=widgets.Layout(width='auto', margin='20px 0 0 0'))\n",
    "save_prompt_button = widgets.Button(description=\"Save Prompt\", button_style='info', layout=widgets.Layout(width='auto'), disabled=True)\n",
    "save_report_button = widgets.Button(description=\"Save Report\", button_style='info', layout=widgets.Layout(width='auto'), disabled=True)\n",
    "buttons_hbox = widgets.HBox([generate_button, save_prompt_button, save_report_button])\n",
    "output_area = widgets.Output()\n",
    "\n",
    "human_report_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Paste human-written report here for comparison...',\n",
    "    description='Human Report:',\n",
    "    layout=widgets.Layout(width='90%', height='200px')\n",
    ")\n",
    "\n",
    "score_widgets = {\n",
    "    \"executive_summary_score\": widgets.IntSlider(description=\"Exec Summary Score (1-10):\", min=1, max=10, value=5),\n",
    "    \"financial_analysis_score\": widgets.IntSlider(description=\"Financial Analysis Score (1-10):\", min=1, max=10, value=5),\n",
    "    \"qualitative_factors_score\": widgets.IntSlider(description=\"Qualitative Score (1-10):\", min=1, max=10, value=5),\n",
    "    \"rating_rationale_score\": widgets.IntSlider(description=\"Rating Rationale Score (1-10):\", min=1, max=10, value=5),\n",
    "    \"overall_llm_score\": widgets.IntSlider(description=\"Overall LLM Report Score (1-10):\", min=1, max=10, value=5)\n",
    "}\n",
    "feedback_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Provide overall feedback on the LLM report quality, gaps, strengths...',\n",
    "    description='Analyst Feedback:',\n",
    "    layout=widgets.Layout(width='90%', height='100px')\n",
    ")\n",
    "compare_button = widgets.Button(description=\"Generate Comparison & Store Feedback\", button_style='info', layout=widgets.Layout(width='auto'))\n",
    "evaluation_output_area = widgets.Output()\n",
    "ml_data_output_area = widgets.Output()\n",
    "feedback_section = widgets.VBox([\n",
    "    HTML(\"<hr><h2>Evaluation & Feedback Loop</h2>\"),\n",
    "    HTML(\"<p>After generating the LLM report, you can optionally provide a human-written report, score the LLM's output, and give feedback.</p>\"),\n",
    "    HTML(\"<h3>1. Paste Human-Written Report (Optional)</h3>\"),\n",
    "    human_report_input,\n",
    "    HTML(\"<h3>2. Score LLM Report Sections</h3>\"),\n",
    "    *score_widgets.values(),\n",
    "    HTML(\"<h3>3. Provide Qualitative Feedback on LLM Report</h3>\"),\n",
    "    feedback_input,\n",
    "    compare_button,\n",
    "    evaluation_output_area,\n",
    "    ml_data_output_area\n",
    "])\n",
    "feedback_section.layout.display = 'none'\n",
    "\n",
    "def extract_text_from_upload(file_upload_widget_value):\n",
    "    if not file_upload_widget_value:\n",
    "        return \"\", \"No file\"\n",
    "    \n",
    "    uploaded_file_info = list(file_upload_widget_value.values())[0]\n",
    "    file_name = uploaded_file_info['metadata']['name']\n",
    "    content = uploaded_file_info['content']\n",
    "\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if file_name.lower().endswith('.pdf') and pdfium:\n",
    "            pdf_doc = pdfium.PdfDocument(content)\n",
    "            for i in range(len(pdf_doc)):\n",
    "                page = pdf_doc.get_page(i)\n",
    "                text += page.get_textpage().get_text_range() + \"\\n\"\n",
    "            pdf_doc.close()\n",
    "        elif file_name.lower().endswith('.txt'):\n",
    "            text = content.decode('utf-8')\n",
    "        else:\n",
    "            return f\"Unsupported file type: {file_name}\", file_name\n",
    "        return text, file_name\n",
    "    except Exception as e:\n",
    "        return f\"Error processing file {file_name}: {e}\", file_name\n",
    "\n",
    "def call_openai_llm(prompt_text, api_key, model=\"gpt-3.5-turbo\"):\n",
    "    if not OpenAI:\n",
    "        return \"OpenAI library not installed.\"\n",
    "    if not api_key:\n",
    "        return \"API key not provided.\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant generating a corporate credit report.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error calling OpenAI API: {e}\"\n",
    "\n",
    "def create_download_link(data, filename, button_text):\n",
    "    if isinstance(data, str):\n",
    "        b64 = base64.b64encode(data.encode()).decode()\n",
    "    elif isinstance(data, bytes):\n",
    "        b64 = base64.b64encode(data).decode()\n",
    "    else:\n",
    "        json_str = json.dumps(data, indent=2)\n",
    "        b64 = base64.b64encode(json_str.encode()).decode()\n",
    "\n",
    "    payload = f'<a download=\"{filename}\" href=\"data:application/octet-stream;base64,{b64}\" target=\"_blank\">{button_text}</a>'\n",
    "    return widgets.HTML(payload)\n",
    "\n",
    "def on_file_upload_change(change):\n",
    "    if change['new']:\n",
    "        file_info = list(change['new'].values())[0]\n",
    "        uploaded_file_name_display.value = f\"Uploaded: {file_info['metadata']['name']}\"\n",
    "    else:\n",
    "        uploaded_file_name_display.value = \"No file uploaded.\"\n",
    "\n",
    "file_upload_widget.observe(on_file_upload_change, names='value')\n",
    "\n",
    "\n",
    "def on_generate_button_clicked(b):\n",
    "    global current_llm_prompt, current_llm_report_md, current_simulated_report_md, current_user_inputs, current_uploaded_text\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        display(HTML(\"<h4>Processing...</h4>\"))\n",
    "\n",
    "        current_user_inputs = {name: w.value for name, w in input_widgets.items()}\n",
    "\n",
    "        uploaded_text, uploaded_filename = extract_text_from_upload(file_upload_widget.value)\n",
    "        current_uploaded_text = uploaded_text if \"Error\" not in uploaded_text else \"\"\n",
    "        current_user_inputs['uploaded_document_text'] = current_uploaded_text\n",
    "        \n",
    "        display(HTML(f\"<b>Uploaded File:</b> {uploaded_filename}\"))\n",
    "        if \"Error\" in uploaded_text:\n",
    "            display(HTML(f\"<p style='color:red;'>{uploaded_text}</p>\"))\n",
    "        elif current_uploaded_text:\n",
    "             display(HTML(f\"<details><summary>Click to view extracted text (first 500 chars)</summary><p>{current_uploaded_text[:500]}...</p></details>\"))\n",
    "\n",
    "\n",
    "        input_data_prompt_section = \"**USER-PROVIDED INFORMATION:**\\n\\n\"\n",
    "        input_data_prompt_section += f\"Company Name: {current_user_inputs.get('company_name', 'N/A')}\\n\"\n",
    "        input_data_prompt_section += f\"Ticker Symbol: {current_user_inputs.get('company_ticker', 'N/A')}\\n\"\n",
    "        input_data_prompt_section += f\"Primary Sector: {current_user_inputs.get('company_sector', 'N/A')}\\n\\n\"\n",
    "        input_data_prompt_section += \"**Key Analyst Assumptions:**\\n\"\n",
    "        input_data_prompt_section += f\"{current_user_inputs.get('key_assumptions', 'N/A')}\\n\\n\"\n",
    "        input_data_prompt_section += \"**Financial Data:**\\n\"\n",
    "        for name, w_val in current_user_inputs.items():\n",
    "            if name.startswith('financial_'): input_data_prompt_section += f\"- {name.replace('financial_', '').replace('_', ' ').title()}: {w_val}\\n\"\n",
    "        input_data_prompt_section += \"\\n\"\n",
    "        input_data_prompt_section += \"**Credit Metrics:**\\n\"\n",
    "        for name, w_val in current_user_inputs.items():\n",
    "            if name.startswith('metric_'): input_data_prompt_section += f\"- {name.replace('metric_', '').replace('_', ' ').upper()}: {w_val}\\n\"\n",
    "        input_data_prompt_section += \"\\n\"\n",
    "        input_data_prompt_section += \"**Qualitative Factors:**\\n\"\n",
    "        input_data_prompt_section += f\"- Business Description: {current_user_inputs.get('qualitative_business_description', 'N/A')}\\n\"\n",
    "        input_data_prompt_section += f\"- Management & Strategy: {current_user_inputs.get('qualitative_management_strategy', 'N/A')}\\n\"\n",
    "        input_data_prompt_section += f\"- Competitive Landscape: {current_user_inputs.get('qualitative_competitive_landscape', 'N/A')}\\n\"\n",
    "        input_data_prompt_section += f\"- Industry Outlook: {current_user_inputs.get('qualitative_industry_outlook', 'N/A')}\\n\"\n",
    "        input_data_prompt_section += f\"- ESG Factors: {current_user_inputs.get('qualitative_esg_factors', 'N/A')}\\n\\n\"\n",
    "        input_data_prompt_section += \"**Recent News / Press Releases:**\\n\"\n",
    "        input_data_prompt_section += f\"{current_user_inputs.get('recent_news_summary', 'N/A')}\\n\\n\"\n",
    "        \n",
    "        if current_uploaded_text:\n",
    "             input_data_prompt_section += f\"**Additional Context from Uploaded Document ({uploaded_filename}):**\\n\"\n",
    "             input_data_prompt_section += f\"{current_uploaded_text[:2000]}\\n\"\n",
    "        else:\n",
    "             input_data_prompt_section += f\"**Additional Context from Uploaded Document:**\\nNot provided.\\n\"\n",
    "\n",
    "\n",
    "        current_llm_prompt = PROMPT_TEMPLATE_CORE.replace(\"{company_name}\", current_user_inputs.get('company_name', 'N/A'))\n",
    "        current_llm_prompt = current_llm_prompt.replace(\"{company_ticker}\", current_user_inputs.get('company_ticker', 'N/A'))\n",
    "        current_llm_prompt = current_llm_prompt.replace(\"{company_sector}\", current_user_inputs.get('company_sector', 'N/A'))\n",
    "        current_llm_prompt = current_llm_prompt.replace(\"{uploaded_document_context}\", current_uploaded_text[:2000] if current_uploaded_text else \"Not provided.\")\n",
    "        current_llm_prompt = current_llm_prompt.replace(\"---INPUT_DATA_MARKER---\", input_data_prompt_section + \"\\n---\\n\")\n",
    "        current_llm_prompt = current_llm_prompt.replace(\"{report_structure_guide}\", REPORT_STRUCTURE_GUIDE)\n",
    "\n",
    "        display(HTML(\"<h2>Generated LLM Prompt:</h2>\"))\n",
    "        display(Markdown(f\"<details><summary>Click to view full LLM prompt</summary>\\n\\n```text\\n{current_llm_prompt}\\n```\\n\\n</details>\"))\n",
    "\n",
    "        display(HTML(\"<h2>Simulated Credit Report (Rule-based):</h2>\"))\n",
    "        current_simulated_report_md = report_simulator.generate_simulated_report(current_user_inputs)\n",
    "        display(Markdown(current_simulated_report_md))\n",
    "        sim_report_download_link = create_download_link(current_simulated_report_md, f\"{current_user_inputs.get('company_ticker','SIM')}_simulated_report.md\", \"Download Simulated Report (MD)\")\n",
    "        display(sim_report_download_link)\n",
    "\n",
    "\n",
    "        display(HTML(\"<h2>LLM-Generated Credit Report:</h2>\"))\n",
    "        api_key = api_key_input.value\n",
    "        selected_model = llm_model_select.value\n",
    "        if not api_key:\n",
    "            display(HTML(\"<p style='color:red;'>OpenAI API Key not provided. LLM report generation skipped.</p>\"))\n",
    "            current_llm_report_md = \"API Key not provided. LLM report not generated.\"\n",
    "        elif not OpenAI:\n",
    "             display(HTML(\"<p style='color:red;'>OpenAI library not installed. LLM report generation skipped.</p>\"))\n",
    "             current_llm_report_md = \"OpenAI library not installed. LLM report not generated.\"\n",
    "        else:\n",
    "            display(HTML(f\"<p><i>Calling LLM ({selected_model})... This may take a moment.</i></p>\"))\n",
    "            current_llm_report_md = call_openai_llm(current_llm_prompt, api_key, model=selected_model)\n",
    "            display(Markdown(current_llm_report_md))\n",
    "            llm_report_download_link = create_download_link(current_llm_report_md, f\"{current_user_inputs.get('company_ticker','LLM')}_{selected_model}_report.md\", f\"Download LLM Report ({selected_model}) (MD)\")\n",
    "            display(llm_report_download_link)\n",
    "\n",
    "        if current_llm_report_md and \"Error\" not in current_llm_report_md and \"not provided\" not in current_llm_report_md :\n",
    "            llm_report_jsonl_object = {\n",
    "                \"reportId\": f\"{current_user_inputs.get('company_ticker', 'unknown').lower()}-llm-{selected_model}-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "                \"generationTimestamp\": datetime.datetime.now().isoformat(),\n",
    "                \"reportDate\": datetime.date.today().isoformat(),\n",
    "                \"companyName\": current_user_inputs.get('company_name'),\n",
    "                \"companyTicker\": current_user_inputs.get('company_ticker'),\n",
    "                \"companySector\": current_user_inputs.get('company_sector'),\n",
    "                \"llmModelUsed\": selected_model,\n",
    "                \"fullPrompt\": current_llm_prompt,\n",
    "                \"reportMarkdown\": current_llm_report_md,\n",
    "                \"sourceSystem\": \"InteractiveCreditReportNotebook_v2_LLM\"\n",
    "            }\n",
    "            jsonl_download_link = create_download_link(json.dumps(llm_report_jsonl_object), f\"{current_user_inputs.get('company_ticker','LLM')}_{selected_model}_report.jsonl\", f\"Download LLM Report ({selected_model}) (JSONL)\")\n",
    "            display(jsonl_download_link)\n",
    "        \n",
    "        save_prompt_button.disabled = False\n",
    "        save_report_button.disabled = False\n",
    "        feedback_section.layout.display = 'block'\n",
    "\n",
    "def on_save_prompt_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_format = output_format_dropdown.value.lower()\n",
    "        company_name = input_widgets['company_name'].value.replace(' ', '_')\n",
    "        filename = f\"../prompt_library/{company_name}_prompt.{output_format}\"\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        if output_format == 'markdown':\n",
    "            content = current_llm_prompt\n",
    "        elif output_format == 'json':\n",
    "            content = json.dumps({'prompt': current_llm_prompt}, indent=4)\n",
    "        elif output_format == 'yaml':\n",
    "            content = yaml.dump({'prompt': current_llm_prompt}, default_flow_style=False)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(content)\n",
    "        display(HTML(f\"<b>Prompt saved to {filename}</b>\"))\n",
    "\n",
    "def on_save_report_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_format = output_format_dropdown.value.lower()\n",
    "        company_name = input_widgets['company_name'].value.replace(' ', '_')\n",
    "        filename = f\"../prompt_library/{company_name}_report.{output_format}\"\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        if output_format == 'markdown':\n",
    "            content = current_simulated_report_md\n",
    "        elif output_format == 'json':\n",
    "            content = json.dumps({'report': current_simulated_report_md}, indent=4)\n",
    "        elif output_format == 'yaml':\n",
    "            content = yaml.dump({'report': current_simulated_report_md}, default_flow_style=False)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(content)\n",
    "        display(HTML(f\"<b>Report saved to {filename}</b>\"))\n",
    "\n",
    "def on_compare_button_clicked(b):\n",
    "    global current_llm_report_md, current_user_inputs, current_llm_prompt, current_uploaded_text\n",
    "    with evaluation_output_area:\n",
    "        evaluation_output_area.clear_output(wait=True)\n",
    "        display(HTML(\"<h4>Processing Comparison and Feedback...</h4>\"))\n",
    "\n",
    "        human_report_text = human_report_input.value\n",
    "        analyst_feedback_text = feedback_input.value\n",
    "        scores = {name: w.value for name, w in score_widgets.items()}\n",
    "\n",
    "        if not current_llm_report_md or \"Error\" in current_llm_report_md or \"not generated\" in current_llm_report_md:\n",
    "            display(HTML(\"<p style='color:red;'>LLM report not available for comparison. Please generate it first.</p>\"))\n",
    "            return\n",
    "\n",
    "        comparison_html = \"<h3>Report Comparison</h3>\"\n",
    "        comparison_html += \"<table border='1' style='width:100%; border-collapse: collapse;'><tr><th>Aspect</th><th>LLM Report Snippet (Sample)</th><th>Human Report Snippet (Sample)</th><th>Score</th></tr>\"\n",
    "\n",
    "        llm_exec_summary_sample = current_llm_report_md.split(\"## 1. Executive Summary\")[1].split(\"## 2.\")[0][:300] if \"## 1. Executive Summary\" in current_llm_report_md else \"N/A\"\n",
    "        human_exec_summary_sample = human_report_text.split(\"## 1. Executive Summary\")[1].split(\"## 2.\")[0][:300] if \"## 1. Executive Summary\" in human_report_text else \"N/A\"\n",
    "        comparison_html += f\"<tr><td>Executive Summary</td><td><pre>{llm_exec_summary_sample}...</pre></td><td><pre>{human_exec_summary_sample}...</pre></td><td>{scores.get('executive_summary_score','N/A')}/10</td></tr>\"\n",
    "        \n",
    "        comparison_html += f\"<tr><td colspan='3'><b>Overall LLM Report Score</b></td><td><b>{scores.get('overall_llm_score','N/A')}/10</b></td></tr>\"\n",
    "        comparison_html += \"</table>\"\n",
    "        display(HTML(comparison_html))\n",
    "\n",
    "        display(HTML(\"<h3>Analyst Feedback Provided:</h3>\"))\n",
    "        display(Markdown(f\"> {analyst_feedback_text if analyst_feedback_text else 'No feedback provided.'}\"))\n",
    "\n",
    "\n",
    "        ml_training_data_entry = {\n",
    "            \"record_id\": f\"feedback-{current_user_inputs.get('company_ticker', 'unknown').lower()}-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S%f')}\",\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"user_inputs\": current_user_inputs,\n",
    "            \"uploaded_document_text_snippet\": current_uploaded_text[:1000] if current_uploaded_text else None,\n",
    "            \"llm_prompt\": current_llm_prompt,\n",
    "            \"llm_model_used\": llm_model_select.value,\n",
    "            \"llm_generated_report_markdown\": current_llm_report_md,\n",
    "            \"human_written_report_markdown\": human_report_text if human_report_text else None,\n",
    "            \"comparison_scores\": scores,\n",
    "            \"analyst_qualitative_feedback\": analyst_feedback_text if analyst_feedback_text else None\n",
    "        }\n",
    "\n",
    "        ml_data_filename = \"credit_analysis_ml_feedback_data.jsonl\"\n",
    "        try:\n",
    "            with open(ml_data_filename, \"a\") as f:\n",
    "                f.write(json.dumps(ml_training_data_entry) + \"\\n\")\n",
    "            display(HTML(f\"<p>Feedback and scoring data appended to <code>{ml_data_filename}</code>.</p>\"))\n",
    "            if os.path.exists(ml_data_filename):\n",
    "                 ml_file_download_link = FileLink(ml_data_filename, result_html_prefix=\"Download accumulated ML data: \")\n",
    "                 display(ml_file_download_link)\n",
    "\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<p style='color:red;'>Error saving ML data: {e}</p>\"))\n",
    "            display(HTML(\"<p>You can copy the ML data entry below:</p>\"))\n",
    "            display(Markdown(f\"```json\\n{json.dumps(ml_training_data_entry, indent=2)}\\n```\"))\n",
    "\n",
    "generate_button.on_click(on_generate_button_clicked)\n",
    "save_prompt_button.on_click(on_save_prompt_button_clicked)\n",
    "save_report_button.on_click(on_save_report_button_clicked)\n",
    "compare_button.on_click(on_compare_button_clicked)\n",
    "\n",
    "display(HTML(\"<h2>LLM Configuration</h2>\"))\n",
    "display(api_key_input)\n",
    "display(llm_model_select)\n",
    "\n",
    "display(HTML(\"<h2>Data Inputs</h2>\"))\n",
    "display(HTML(\"<h3>1. Upload Supporting Document (Optional PDF/TXT)</h3>\"))\n",
    "display(file_upload_widget)\n",
    "display(uploaded_file_name_display)\n",
    "\n",
    "display(HTML(\"<h3>2. Enter Company & Analysis Data</h3>\"))\n",
    "display(all_inputs_vbox)\n",
    "display(output_format_dropdown)\n",
    "display(buttons_hbox)\n",
    "display(output_area)\n",
    "display(feedback_section)"
   ]
  }
 ]
}
